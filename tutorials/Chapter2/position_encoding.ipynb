{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绝对位置编码——sinusoidal\n",
    "\n",
    "> 出自论文 `Attention is all your need`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        encodings = self.get_positional_encoding(d_model, max_len)\n",
    "        self.register_buffer('positional_encodings', encodings, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_positional_encoding(d_model: int, max_len: int):\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "        div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
    "        encodings = torch.zeros(max_len, d_model)\n",
    "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "        return encodings.unsqueeze(0).requires_grad_(False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pe = self.positional_encodings[:x.shape[1]].detach().requires_grad_(False)\n",
    "        return self.dropout(x + pe)\n",
    "\n",
    "def _test_positional_encoding():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    pe = PositionalEncoding.get_positional_encoding(20, 100)\n",
    "    print(pe.shape)\n",
    "    plt.plot(np.arange(100), pe[:, 0, 4:8].numpy())\n",
    "    plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "    plt.title(\"Positional encoding\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _test_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相对位置编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
