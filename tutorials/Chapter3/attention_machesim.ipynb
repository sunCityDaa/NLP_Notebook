{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention （self-attention被作为特例）\n",
    "假设 batch size = $B$，序列长度 = $L$，隐藏维度 = $d_{model}$，head 数 = $h$，每个 head 的维度 =$d_k = d_v = d_{model} / h$：\n",
    "\n",
    "1. **输入向量（比如 embedding 或上一层输出）：**\n",
    "    \n",
    "    $Q \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}, K \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}},V \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}},$\n",
    "    \n",
    "\t如果是self-attention，则$Q=K=V$，而对于cross attention，则$Q$，$K$， $V$不同\n",
    "    \n",
    "2. **对每个 head，使用独立的参数矩阵：**\n",
    "\t\n",
    "\t$Q = Q W_Q, \\quad K = K W_K, \\quad V = V W_V$\n",
    "\n",
    "\t其中 $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{model}}$， 计算后的$Q, K, V \\in \\mathbb{R}^{B \\times L \\times d_{model}}$\n",
    "\n",
    "\t将$Q,K,V$ 按照$d_{model}$切分为$h$个head，每个维度为 $d_k$，因此$d_{model} = h\\cdot d_k$，那么$Q,K,V$ 的维度变为：\n",
    "\n",
    "\t$Q, K, V \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$\n",
    "\n",
    "3.  **计算 Attention Score**\n",
    "\n",
    "\t单个 head 的注意力：\n",
    "\t$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "\t其中  $Q \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$， $K^\\top \\in \\mathbb{R}^{B \\times h \\times d_k \\times L}$， $QK^\\top \\in \\mathbb{R}^{B \\times h \\times L \\times L}$，softmax 后得到注意力权重：$\\in \\mathbb{R}^{B \\times h \\times L \\times L}$， 乘上 $V \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$，输出结果：$\\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$\n",
    " \n",
    " 4. **拼接所有头**\n",
    "\n",
    "\t将 $h$ 个 head 的结果拼接：\n",
    "\n",
    "\t$\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\in \\mathbb{R}^{B \\times h \\times L \\times d_k} \\rightarrow  \\mathbb{R}^{B \\times L \\times (h \\cdot d_k)} = \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}$\n",
    "\n",
    "5. **输出线性层**\n",
    "\n",
    "\t最后乘上一个输出变换矩阵：\n",
    "\n",
    "\t$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O$\n",
    "\n",
    "\t其中 $W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$，$\\text{MHA}(X) \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公式总结 \n",
    "\n",
    "$\\begin{aligned} Q &= Q W_Q \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ K &= K W_K \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ V &= V W_V \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ \\text{Attention}(Q,K,V) &= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\quad (B,h,L,d_k)\\times(B,h,d_k,L) \\times (B,h,L,d_k)\\to (B,h,L,d_k)  \\\\ \\text{MHA}(Q,K,V) &= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O \\quad  \\{(B,h,L,d_k) \\to  (B,L,h,d_k) \\to (B,L,h \\cdot d_v)\\}\\times(d_{\\text{model}},d_{\\text{model}}) \\to (B,L,d_{\\text{model}}) \\end{aligned}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # embedding特征大小\n",
    "        self.h = h  # 头的个数\n",
    "        # 确保d_model可以被h整除\n",
    "        assert d_model % h == 0, \"d_model 不能被 h整除\"\n",
    "\n",
    "        self.d_k = d_model // h  # 每个头特征大小\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    @staticmethod \n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # 给mask为0的位置填入一个很大的负值\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 计算注意力\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # 多个头合并\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # 乘以输出层\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "！！！ 下面所有的代码都是参考 [MHA, GQA, MQA, MLA的代码 - 文举的博客](https://liwenju0.com/posts/MHA,-GQA,-MQA,-MLA%E7%9A%84%E4%BB%A3%E7%A0%81.html)\n",
    "\n",
    "下面是此博客中关于MHA的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim必须能被num_heads整除\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # 线性变换并分头\n",
    "        # [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 转置为 [batch_size, num_heads, seq_len, head_dim]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数 (缩放点积注意力)\n",
    "        # [batch_size, num_heads, query_len, head_dim] x [batch_size, num_heads, head_dim, key_len]\n",
    "        # -> [batch_size, num_heads, query_len, key_len]\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if attn_mask is not None:\n",
    "            attn_weights = attn_weights + attn_mask\n",
    "            \n",
    "        if key_padding_mask is not None:\n",
    "            # 扩展key_padding_mask到合适的维度\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n",
    "        \n",
    "        # softmax归一化\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 计算输出 [batch_size, num_heads, query_len, key_len] x [batch_size, num_heads, value_len, head_dim]\n",
    "        # -> [batch_size, num_heads, query_len, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # 转置并重新形状化 [batch_size, num_heads, query_len, head_dim] -> [batch_size, query_len, embed_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # 最终的线性变换\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"前馈神经网络，包含两个线性层，中间有激活函数\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.0, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 支持不同的激活函数\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'silu' or activation == 'swish':\n",
    "            self.activation = F.silu\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Transformer编码器层，包含多头注意力和前馈神经网络\"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim, \n",
    "                 num_heads, \n",
    "                 ffn_dim=2048, \n",
    "                 dropout=0.1, \n",
    "                 activation='relu',\n",
    "                 norm_first=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 多头注意力\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # 前馈神经网络\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, dropout, activation)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 是否先进行归一化（Pre-LN）还是后归一化（Post-LN）\n",
    "        self.norm_first = norm_first\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # 是否先归一化取决于norm_first参数\n",
    "        if self.norm_first:\n",
    "            # Pre-LN\n",
    "            attn_output = self._sa_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            src = src + attn_output\n",
    "            src = src + self._ff_block(self.norm2(src))\n",
    "        else:\n",
    "            # Post-LN (原始Transformer架构)\n",
    "            attn_output = self._sa_block(src, src_mask, src_key_padding_mask)\n",
    "            src = self.norm1(src + attn_output)\n",
    "            src = self.norm2(src + self._ff_block(src))\n",
    "        \n",
    "        return src\n",
    "    \n",
    "    # 自注意力块\n",
    "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
    "        x, _ = self.self_attn(x, x, x, attn_mask, key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    # 前馈网络块\n",
    "    def _ff_block(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现注意点\n",
    "\n",
    "- 关于mask，在对attention score进行softmax前，需要将attention score 将掩码为 0 的位置填充为一个极小值（-1e9）,或者-inf，那么在进行softmax 后该位置的概率趋近于 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MQA && GQA\n",
    "\n",
    "在公式层面和MHA差别不大，MHA以及MQA都可以算是GQA的特例，下面代码是GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 如果num_kv_head = 1,就是MQA，如果num_kv_head = num_head，就是标准的多头注意力，也即MHA，中间的值是GQA\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, num_kv_heads=None):  # 添加num_kv_heads参数\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim必须能被num_heads整除\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # GQA相关修改：添加num_kv_heads参数处理\n",
    "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n",
    "        assert self.num_heads % self.num_kv_heads == 0, \"num_heads必须能被num_kv_heads整除\"\n",
    "        # 每个KV头服务的Q头数量\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # 修改K和V的投影维度为num_kv_heads * head_dim\n",
    "        self.k_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # 线性变换\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        # 分头，q保持原来的头数，k和v使用较少的头数\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        # 转置维度\n",
    "        q = q.transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "        \n",
    "        # GQA核心修改：扩展k和v以匹配q的头数\n",
    "        # 每个kv头会被复制num_queries_per_kv次以匹配query头的数量\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            k = k.repeat_interleave(self.num_queries_per_kv, dim=1)  # 重复扩展k\n",
    "            v = v.repeat_interleave(self.num_queries_per_kv, dim=1)  # 重复扩展v\n",
    "        \n",
    "        # 计算注意力分数 (缩放点积注意力)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if attn_mask is not None:\n",
    "            attn_weights = attn_weights + attn_mask\n",
    "            \n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n",
    "        \n",
    "        # softmax归一化\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 计算输出\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # 转置并重新形状化\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # 最终的线性变换\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"前馈神经网络，包含两个线性层，中间有激活函数\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.0, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 支持不同的激活函数\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'silu' or activation == 'swish':\n",
    "            self.activation = F.silu\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Transformer编码器层，包含多头注意力和前馈神经网络\"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim, \n",
    "                 num_heads, \n",
    "                 ffn_dim=2048, \n",
    "                 dropout=0.1, \n",
    "                 activation='relu',\n",
    "                 norm_first=False,\n",
    "                 num_kv_heads=None):  # 添加num_kv_heads参数\n",
    "        super().__init__()\n",
    "        \n",
    "        # 多头注意力，传入num_kv_heads参数\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout, num_kv_heads)\n",
    "        \n",
    "        # 前馈神经网络\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, dropout, activation)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 是否先进行归一化（Pre-LN）还是后归一化（Post-LN）\n",
    "        self.norm_first = norm_first\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # 是否先归一化取决于norm_first参数\n",
    "        if self.norm_first:\n",
    "            # Pre-LN\n",
    "            attn_output = self._sa_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            src = src + attn_output\n",
    "            src = src + self._ff_block(self.norm2(src))\n",
    "        else:\n",
    "            # Post-LN (原始Transformer架构)\n",
    "            attn_output = self._sa_block(src, src_mask, src_key_padding_mask)\n",
    "            src = self.norm1(src + attn_output)\n",
    "            src = self.norm2(src + self._ff_block(src))\n",
    "        \n",
    "        return src\n",
    "    \n",
    "    # 自注意力块\n",
    "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
    "        x, _ = self.self_attn(x, x, x, attn_mask, key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    # 前馈网络块\n",
    "    def _ff_block(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return self.dropout2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌼公式解析（实现）\n",
    "\n",
    "\n",
    "公式中$d_{model}$指的是MLA类中的参数$dim$，公式中的$d_k$ 指的是MLA类中的参数$qk\\_head\\_dim$  \n",
    "\n",
    "#### q向量计算\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "c^Q &= h_t W_{DQ} \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{q\\_lora\\_rank}) \\to (B,L,d_{q\\_lora\\_rank}) \\\\\n",
    "q^C &= c^Q W_{UQ} \\quad (B,L,d_{q\\_lora\\_rank}) \\times (d_{q\\_lora\\_rank}, h \\cdot d_{qk\\_nope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_nope\\_head\\_dim}) \\\\\n",
    "q^R &= \\text{RoPE}(c^Q W_{QR}) \\quad (B,L,d_{q\\_lora\\_rank}) \\times (d_{q\\_lora\\_rank}, h \\cdot d_{qk\\_rope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_rope\\_head\\_dim}) \\\\\n",
    "Q &= \\text{Concat}(q^C, q^R) \\to (B,h,L,d_k), \\quad d_k = d_{qk\\_nope\\_head\\_dim} + d_{qk\\_rope\\_head\\_dim}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "#### kv 向量计算\n",
    "\n",
    "$$\\begin{aligned}\n",
    "c^{KV} &= h_t W_{DKV} \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{kv\\_lora\\_rank}) \\to (B,L,d_{kv\\_lora\\_rank}) \\\\\n",
    "k^C &= c^{KV} W_{UK} \\quad (B,L,d_{kv\\_lora\\_rank}) \\times (d_{kv\\_lora\\_rank}, h \\cdot d_{qk\\_nope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_nope\\_head\\_dim}) \\\\\n",
    "k^R &= \\text{RoPE}(h_t W_{KR}) \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, 1 \\cdot d_{qk\\_rope\\_head\\_dim}) \\to (B,L,1,d_{qk\\_rope\\_head\\_dim}) \\to \\text{broadcast} \\to (B,L,h,d_{qk\\_rope\\_head\\_dim}) \\\\\n",
    "K &= \\text{Concat}(k^C, k^R) \\to (B,h,L,d_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "V &= c^{KV} W_{UV} \\quad (B,L,d_{kv\\_lora\\_rank}) \\times (d_{kv\\_lora\\_rank}, h \\cdot d_{v\\_head\\_dim}) \\to (B,L,h,d_{v\\_head\\_dim}) \\to (B,h,L,d_{v\\_head\\_dim})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### self-attention 计算\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Attention}(Q,K,V) \n",
    "&= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\\\\n",
    "&= \\text{softmax}\\!\\left(\\frac{Q^C (K^C)^\\top + Q^R (K^R)^\\top}{\\sqrt{d_k}}\\right)V \\\\\n",
    "&\\to (B,h,L,d_k) \\times (B,h,L,d_k)^\\top \\to (B,h,L,L) \\\\\n",
    "&\\to \\text{softmax}(B,h,L,L) \\times (B,h,L,d_{v\\_head\\_dim}) \\to (B,h,L,d_{v\\_head\\_dim})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{MLA}(Q,K,V) &= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O \\\\\n",
    "&\\to (B,h,L,d_{v\\_head\\_dim}) \\to (B,L,h \\cdot d_{v\\_head\\_dim}) \\times (h \\cdot d_{v\\_head\\_dim}, d_{\\text{model}}) \\to (B,L,d_{\\text{model}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "在计算$q^C (k^C)^\\top$的时候，按照下列的计算方式\n",
    "\n",
    "$q^{C} k^{C^T} = c^Q W_{UQ}(c^{KV} W_{UK})^T = c^Q W_{UQ} (W_{UK})^T (c^{KV})^T =(c^Q W_{UQ} (W_{UK})^T) (c^{KV})^T$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现说明\n",
    "代码中给出了两种计算方法，一种是naive，naive就是上面的公式，而另一种是absorb，则是mla的通用实现，计算效率以及KV cache都更优，\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![MLA](asserts/mla.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 计算RMS Norm\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):\n",
    "    \"\"\"预计算位置编码的复数表示\"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(max_seq_len).float()\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    \"\"\"应用旋转位置编码\"\"\"\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.view(1, x_complex.size(1), 1, x_complex.size(-1))\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs_cis).flatten(3)\n",
    "    return x_rotated.type_as(x)\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    \"\"\"多头潜在注意力（Multi-head Latent Attention）\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 n_heads, \n",
    "                 qk_nope_head_dim=128,  # 非位置编码部分的头维度\n",
    "                 qk_rope_head_dim=64,   # 位置编码部分的头维度\n",
    "                 v_head_dim=128,        # 值的头维度\n",
    "                 q_lora_rank=0,         # 查询的低秩投影维度\n",
    "                 kv_lora_rank=512,      # 键值的低秩投影维度\n",
    "                 dropout=0.0,\n",
    "                 attn_impl=\"naive\"):    # 注意力实现方式：naive或absorb\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.qk_nope_head_dim = qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = qk_rope_head_dim\n",
    "        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n",
    "        self.v_head_dim = v_head_dim\n",
    "        self.q_lora_rank = q_lora_rank\n",
    "        self.kv_lora_rank = kv_lora_rank\n",
    "        self.attn_impl = attn_impl\n",
    "        \n",
    "        # 计算注意力缩放因子\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        \n",
    "        # 针对查询的投影层 - 可选使用低秩投影\n",
    "        if self.q_lora_rank == 0:\n",
    "            # 直接投影\n",
    "            self.wq = nn.Linear(dim, n_heads * self.qk_head_dim)\n",
    "        else:\n",
    "            # 使用低秩投影 (LoRA)\n",
    "            self.wq_a = nn.Linear(dim, self.q_lora_rank)\n",
    "            self.q_norm = RMSNorm(self.q_lora_rank)\n",
    "            self.wq_b = nn.Linear(self.q_lora_rank, n_heads * self.qk_head_dim)\n",
    "        \n",
    "        # 键值使用低秩投影\n",
    "        self.wkv_a = nn.Linear(dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n",
    "        \n",
    "        # 输出投影\n",
    "        self.wo = nn.Linear(n_heads * self.v_head_dim, dim)\n",
    "        \n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, start_pos, freqs_cis, attention_mask=None, max_seq_len=4096):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        end_pos = start_pos + seq_len\n",
    "        \n",
    "        # 生成q向量 - 可选使用低秩投影\n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        else:\n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x)))\n",
    "        \n",
    "        # 将q分为非位置编码部分和位置编码部分\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        \n",
    "        # 应用旋转位置编码到q_pe\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "        \n",
    "        # 生成kv向量 - 使用低秩投影\n",
    "        kv = self.wkv_a(x)\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n",
    "        \n",
    "        # 使用naive方式实现注意力\n",
    "        if self.attn_impl == \"naive\":\n",
    "            # 完整的query向量\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1)\n",
    "            \n",
    "            # 通过低秩投影生成键值\n",
    "            kv = self.wkv_b(self.kv_norm(kv))\n",
    "            kv = kv.view(batch_size, seq_len, self.n_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "            \n",
    "            # 完整的key向量\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_heads, -1)], dim=-1)\n",
    "            \n",
    "            # 创建KV缓存\n",
    "            k_cache = torch.zeros(batch_size, max_seq_len, self.n_heads, self.qk_head_dim, device=x.device, dtype=x.dtype)\n",
    "            v_cache = torch.zeros(batch_size, max_seq_len, self.n_heads, self.v_head_dim, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            # 更新KV缓存\n",
    "            k_cache[:, start_pos:end_pos] = k\n",
    "            v_cache[:, start_pos:end_pos] = v\n",
    "            \n",
    "            # 计算注意力分数\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q, k_cache[:, :end_pos]) * self.softmax_scale\n",
    "            \n",
    "        else:  # absorb方式实现注意力\n",
    "            # 获取wkv_b权重\n",
    "            wkv_b = self.wkv_b.weight.view(self.n_heads, -1, self.kv_lora_rank)\n",
    "            \n",
    "            # 计算q_nope与权重的点积\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim])\n",
    "            \n",
    "            # 创建KV缓存\n",
    "            kv_cache = torch.zeros(batch_size, max_seq_len, self.kv_lora_rank, device=x.device, dtype=x.dtype)\n",
    "            pe_cache = torch.zeros(batch_size, max_seq_len, self.qk_rope_head_dim, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            # 更新KV缓存\n",
    "            kv_cache[:, start_pos:end_pos] = self.kv_norm(kv)\n",
    "            pe_cache[:, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "            \n",
    "            # 计算注意力分数 - 分别计算非位置编码部分和位置编码部分\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, kv_cache[:, :end_pos]) + \n",
    "                     torch.einsum(\"bshr,btr->bsht\", q_pe, pe_cache[:, :end_pos])) * self.softmax_scale\n",
    "        \n",
    "        # 应用注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            scores += attention_mask.unsqueeze(1)\n",
    "        \n",
    "        # 注意力权重计算\n",
    "        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).type_as(x)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 计算输出\n",
    "        if self.attn_impl == \"naive\":\n",
    "            output = torch.einsum(\"bsht,bthd->bshd\", attn_weights, v_cache[:, :end_pos])\n",
    "        else:\n",
    "            # 先与kv_cache相乘\n",
    "            output = torch.einsum(\"bsht,btc->bshc\", attn_weights, kv_cache[:, :end_pos])\n",
    "            # 再与权重相乘生成最终输出\n",
    "            output = torch.einsum(\"bshc,hdc->bshd\", output, wkv_b[:, -self.v_head_dim:])\n",
    "        \n",
    "        # 重塑并投影到原始维度\n",
    "        output = output.reshape(batch_size, seq_len, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "class MLABlock(nn.Module):\n",
    "    \"\"\"包含MLA注意力机制的Transformer块\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim=768, \n",
    "                 n_heads=12, \n",
    "                 qk_nope_head_dim=128, \n",
    "                 qk_rope_head_dim=64, \n",
    "                 v_head_dim=128,\n",
    "                 q_lora_rank=0, \n",
    "                 kv_lora_rank=512, \n",
    "                 mlp_ratio=4, \n",
    "                 dropout=0.1, \n",
    "                 attn_impl=\"naive\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 注意力层\n",
    "        self.attention = MLA(\n",
    "            dim=dim,\n",
    "            n_heads=n_heads,\n",
    "            qk_nope_head_dim=qk_nope_head_dim,\n",
    "            qk_rope_head_dim=qk_rope_head_dim,\n",
    "            v_head_dim=v_head_dim,\n",
    "            q_lora_rank=q_lora_rank,\n",
    "            kv_lora_rank=kv_lora_rank,\n",
    "            dropout=dropout,\n",
    "            attn_impl=attn_impl\n",
    "        )\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        \n",
    "    def forward(self, x, start_pos, freqs_cis, attention_mask=None):\n",
    "        # 残差连接 + 注意力层\n",
    "        x = x + self.attention(self.norm1(x), start_pos, freqs_cis, attention_mask)\n",
    "        # 残差连接 + 前馈网络\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# 简单使用示例\n",
    "def mla_example():\n",
    "    # 设置参数\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    dim = 512\n",
    "    n_heads = 8\n",
    "    max_seq_len = 4096\n",
    "    \n",
    "    # 创建输入\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # 计算位置编码\n",
    "    freqs_cis = precompute_freqs_cis(64, max_seq_len)\n",
    "    freqs_cis = freqs_cis[:seq_len]\n",
    "    \n",
    "    # 创建注意力掩码 (因果掩码)\n",
    "    mask = torch.full((seq_len, seq_len), float('-inf')).triu_(1)\n",
    "    \n",
    "    # 创建MLA块\n",
    "    mla_block = MLABlock(\n",
    "        dim=dim,\n",
    "        n_heads=n_heads,\n",
    "        attn_impl=\"absorb\"  # 使用absorb实现\n",
    "    )\n",
    "    \n",
    "    # 前向传播\n",
    "    output = mla_block(x, start_pos=0, freqs_cis=freqs_cis, attention_mask=mask)\n",
    "    \n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "    \n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
