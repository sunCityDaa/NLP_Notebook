{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention ï¼ˆself-attentionè¢«ä½œä¸ºç‰¹ä¾‹ï¼‰\n",
    "å‡è®¾ batch size = $B$ï¼Œåºåˆ—é•¿åº¦ = $L$ï¼Œéšè—ç»´åº¦ = $d_{model}$ï¼Œhead æ•° = $h$ï¼Œæ¯ä¸ª head çš„ç»´åº¦ =$d_k = d_v = d_{model} / h$ï¼š\n",
    "\n",
    "1. **è¾“å…¥å‘é‡ï¼ˆæ¯”å¦‚ embedding æˆ–ä¸Šä¸€å±‚è¾“å‡ºï¼‰ï¼š**\n",
    "    \n",
    "    $Q \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}, K \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}},V \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}},$\n",
    "    \n",
    "\tå¦‚æœæ˜¯self-attentionï¼Œåˆ™$Q=K=V$ï¼Œè€Œå¯¹äºcross attentionï¼Œåˆ™$Q$ï¼Œ$K$ï¼Œ $V$ä¸åŒ\n",
    "    \n",
    "2. **å¯¹æ¯ä¸ª headï¼Œä½¿ç”¨ç‹¬ç«‹çš„å‚æ•°çŸ©é˜µï¼š**\n",
    "\t\n",
    "\t$Q = Q W_Q, \\quad K = K W_K, \\quad V = V W_V$\n",
    "\n",
    "\tå…¶ä¸­ $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{model}}$ï¼Œ è®¡ç®—åçš„$Q, K, V \\in \\mathbb{R}^{B \\times L \\times d_{model}}$\n",
    "\n",
    "\tå°†$Q,K,V$ æŒ‰ç…§$d_{model}$åˆ‡åˆ†ä¸º$h$ä¸ªheadï¼Œæ¯ä¸ªç»´åº¦ä¸º $d_k$ï¼Œå› æ­¤$d_{model} = h\\cdot d_k$ï¼Œé‚£ä¹ˆ$Q,K,V$ çš„ç»´åº¦å˜ä¸ºï¼š\n",
    "\n",
    "\t$Q, K, V \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$\n",
    "\n",
    "3.  **è®¡ç®— Attention Score**\n",
    "\n",
    "\tå•ä¸ª head çš„æ³¨æ„åŠ›ï¼š\n",
    "\t$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "\tå…¶ä¸­  $Q \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$ï¼Œ $K^\\top \\in \\mathbb{R}^{B \\times h \\times d_k \\times L}$ï¼Œ $QK^\\top \\in \\mathbb{R}^{B \\times h \\times L \\times L}$ï¼Œsoftmax åå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼š$\\in \\mathbb{R}^{B \\times h \\times L \\times L}$ï¼Œ ä¹˜ä¸Š $V \\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$ï¼Œè¾“å‡ºç»“æœï¼š$\\in \\mathbb{R}^{B \\times h \\times L \\times d_k}$\n",
    " \n",
    " 4. **æ‹¼æ¥æ‰€æœ‰å¤´**\n",
    "\n",
    "\tå°† $h$ ä¸ª head çš„ç»“æœæ‹¼æ¥ï¼š\n",
    "\n",
    "\t$\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\in \\mathbb{R}^{B \\times h \\times L \\times d_k} \\rightarrow  \\mathbb{R}^{B \\times L \\times (h \\cdot d_k)} = \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}$\n",
    "\n",
    "5. **è¾“å‡ºçº¿æ€§å±‚**\n",
    "\n",
    "\tæœ€åä¹˜ä¸Šä¸€ä¸ªè¾“å‡ºå˜æ¢çŸ©é˜µï¼š\n",
    "\n",
    "\t$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O$\n",
    "\n",
    "\tå…¶ä¸­ $W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$ï¼Œ$\\text{MHA}(X) \\in \\mathbb{R}^{B \\times L \\times d_{\\text{model}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…¬å¼æ€»ç»“ \n",
    "\n",
    "$\\begin{aligned} Q &= Q W_Q \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ K &= K W_K \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ V &= V W_V \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{model}) \\to (B,L,d_{model}) \\to (B,L,h,d_k) \\to (B,h,L,d_k)\\\\ \\text{Attention}(Q,K,V) &= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\quad (B,h,L,d_k)\\times(B,h,d_k,L) \\times (B,h,L,d_k)\\to (B,h,L,d_k)  \\\\ \\text{MHA}(Q,K,V) &= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O \\quad  \\{(B,h,L,d_k) \\to  (B,L,h,d_k) \\to (B,L,h \\cdot d_v)\\}\\times(d_{\\text{model}},d_{\\text{model}}) \\to (B,L,d_{\\text{model}}) \\end{aligned}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # embeddingç‰¹å¾å¤§å°\n",
    "        self.h = h  # å¤´çš„ä¸ªæ•°\n",
    "        # ç¡®ä¿d_modelå¯ä»¥è¢«hæ•´é™¤\n",
    "        assert d_model % h == 0, \"d_model ä¸èƒ½è¢« hæ•´é™¤\"\n",
    "\n",
    "        self.d_k = d_model // h  # æ¯ä¸ªå¤´ç‰¹å¾å¤§å°\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    @staticmethod \n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # ç»™maskä¸º0çš„ä½ç½®å¡«å…¥ä¸€ä¸ªå¾ˆå¤§çš„è´Ÿå€¼\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # è®¡ç®—æ³¨æ„åŠ›\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # å¤šä¸ªå¤´åˆå¹¶\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # ä¹˜ä»¥è¾“å‡ºå±‚\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼ï¼ï¼ ä¸‹é¢æ‰€æœ‰çš„ä»£ç éƒ½æ˜¯å‚è€ƒ [MHA, GQA, MQA, MLAçš„ä»£ç  - æ–‡ä¸¾çš„åšå®¢](https://liwenju0.com/posts/MHA,-GQA,-MQA,-MLA%E7%9A%84%E4%BB%A3%E7%A0%81.html)\n",
    "\n",
    "ä¸‹é¢æ˜¯æ­¤åšå®¢ä¸­å…³äºMHAçš„å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºtransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # å®šä¹‰çº¿æ€§å˜æ¢å±‚\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # çº¿æ€§å˜æ¢å¹¶åˆ†å¤´\n",
    "        # [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # è½¬ç½®ä¸º [batch_size, num_heads, seq_len, head_dim]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›)\n",
    "        # [batch_size, num_heads, query_len, head_dim] x [batch_size, num_heads, head_dim, key_len]\n",
    "        # -> [batch_size, num_heads, query_len, key_len]\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # åº”ç”¨æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰\n",
    "        if attn_mask is not None:\n",
    "            attn_weights = attn_weights + attn_mask\n",
    "            \n",
    "        if key_padding_mask is not None:\n",
    "            # æ‰©å±•key_padding_maskåˆ°åˆé€‚çš„ç»´åº¦\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n",
    "        \n",
    "        # softmaxå½’ä¸€åŒ–\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # è®¡ç®—è¾“å‡º [batch_size, num_heads, query_len, key_len] x [batch_size, num_heads, value_len, head_dim]\n",
    "        # -> [batch_size, num_heads, query_len, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # è½¬ç½®å¹¶é‡æ–°å½¢çŠ¶åŒ– [batch_size, num_heads, query_len, head_dim] -> [batch_size, query_len, embed_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # æœ€ç»ˆçš„çº¿æ€§å˜æ¢\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒåŒ…å«ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œä¸­é—´æœ‰æ¿€æ´»å‡½æ•°\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.0, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # æ”¯æŒä¸åŒçš„æ¿€æ´»å‡½æ•°\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'silu' or activation == 'swish':\n",
    "            self.activation = F.silu\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Transformerç¼–ç å™¨å±‚ï¼ŒåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œ\"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim, \n",
    "                 num_heads, \n",
    "                 ffn_dim=2048, \n",
    "                 dropout=0.1, \n",
    "                 activation='relu',\n",
    "                 norm_first=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # å¤šå¤´æ³¨æ„åŠ›\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # å‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, dropout, activation)\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # æ˜¯å¦å…ˆè¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-LNï¼‰è¿˜æ˜¯åå½’ä¸€åŒ–ï¼ˆPost-LNï¼‰\n",
    "        self.norm_first = norm_first\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # æ˜¯å¦å…ˆå½’ä¸€åŒ–å–å†³äºnorm_firstå‚æ•°\n",
    "        if self.norm_first:\n",
    "            # Pre-LN\n",
    "            attn_output = self._sa_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            src = src + attn_output\n",
    "            src = src + self._ff_block(self.norm2(src))\n",
    "        else:\n",
    "            # Post-LN (åŸå§‹Transformeræ¶æ„)\n",
    "            attn_output = self._sa_block(src, src_mask, src_key_padding_mask)\n",
    "            src = self.norm1(src + attn_output)\n",
    "            src = self.norm2(src + self._ff_block(src))\n",
    "        \n",
    "        return src\n",
    "    \n",
    "    # è‡ªæ³¨æ„åŠ›å—\n",
    "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
    "        x, _ = self.self_attn(x, x, x, attn_mask, key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    # å‰é¦ˆç½‘ç»œå—\n",
    "    def _ff_block(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®ç°æ³¨æ„ç‚¹\n",
    "\n",
    "- å…³äºmaskï¼Œåœ¨å¯¹attention scoreè¿›è¡Œsoftmaxå‰ï¼Œéœ€è¦å°†attention score å°†æ©ç ä¸º 0 çš„ä½ç½®å¡«å……ä¸ºä¸€ä¸ªæå°å€¼ï¼ˆ-1e9ï¼‰,æˆ–è€…-infï¼Œé‚£ä¹ˆåœ¨è¿›è¡Œsoftmax åè¯¥ä½ç½®çš„æ¦‚ç‡è¶‹è¿‘äº 0ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MQA && GQA\n",
    "\n",
    "åœ¨å…¬å¼å±‚é¢å’ŒMHAå·®åˆ«ä¸å¤§ï¼ŒMHAä»¥åŠMQAéƒ½å¯ä»¥ç®—æ˜¯GQAçš„ç‰¹ä¾‹ï¼Œä¸‹é¢ä»£ç æ˜¯GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# å¦‚æœnum_kv_head = 1,å°±æ˜¯MQAï¼Œå¦‚æœnum_kv_head = num_headï¼Œå°±æ˜¯æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›ï¼Œä¹Ÿå³MHAï¼Œä¸­é—´çš„å€¼æ˜¯GQA\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, num_kv_heads=None):  # æ·»åŠ num_kv_headså‚æ•°\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # GQAç›¸å…³ä¿®æ”¹ï¼šæ·»åŠ num_kv_headså‚æ•°å¤„ç†\n",
    "        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n",
    "        assert self.num_heads % self.num_kv_heads == 0, \"num_headså¿…é¡»èƒ½è¢«num_kv_headsæ•´é™¤\"\n",
    "        # æ¯ä¸ªKVå¤´æœåŠ¡çš„Qå¤´æ•°é‡\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "        \n",
    "        # å®šä¹‰çº¿æ€§å˜æ¢å±‚\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # ä¿®æ”¹Kå’ŒVçš„æŠ•å½±ç»´åº¦ä¸ºnum_kv_heads * head_dim\n",
    "        self.k_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # çº¿æ€§å˜æ¢\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        # åˆ†å¤´ï¼Œqä¿æŒåŸæ¥çš„å¤´æ•°ï¼Œkå’Œvä½¿ç”¨è¾ƒå°‘çš„å¤´æ•°\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        # è½¬ç½®ç»´åº¦\n",
    "        q = q.transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "        \n",
    "        # GQAæ ¸å¿ƒä¿®æ”¹ï¼šæ‰©å±•kå’Œvä»¥åŒ¹é…qçš„å¤´æ•°\n",
    "        # æ¯ä¸ªkvå¤´ä¼šè¢«å¤åˆ¶num_queries_per_kvæ¬¡ä»¥åŒ¹é…queryå¤´çš„æ•°é‡\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            k = k.repeat_interleave(self.num_queries_per_kv, dim=1)  # é‡å¤æ‰©å±•k\n",
    "            v = v.repeat_interleave(self.num_queries_per_kv, dim=1)  # é‡å¤æ‰©å±•v\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # åº”ç”¨æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰\n",
    "        if attn_mask is not None:\n",
    "            attn_weights = attn_weights + attn_mask\n",
    "            \n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))\n",
    "        \n",
    "        # softmaxå½’ä¸€åŒ–\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # è®¡ç®—è¾“å‡º\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # è½¬ç½®å¹¶é‡æ–°å½¢çŠ¶åŒ–\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # æœ€ç»ˆçš„çº¿æ€§å˜æ¢\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒåŒ…å«ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œä¸­é—´æœ‰æ¿€æ´»å‡½æ•°\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.0, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # æ”¯æŒä¸åŒçš„æ¿€æ´»å‡½æ•°\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'silu' or activation == 'swish':\n",
    "            self.activation = F.silu\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Transformerç¼–ç å™¨å±‚ï¼ŒåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œ\"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim, \n",
    "                 num_heads, \n",
    "                 ffn_dim=2048, \n",
    "                 dropout=0.1, \n",
    "                 activation='relu',\n",
    "                 norm_first=False,\n",
    "                 num_kv_heads=None):  # æ·»åŠ num_kv_headså‚æ•°\n",
    "        super().__init__()\n",
    "        \n",
    "        # å¤šå¤´æ³¨æ„åŠ›ï¼Œä¼ å…¥num_kv_headså‚æ•°\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout, num_kv_heads)\n",
    "        \n",
    "        # å‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim, dropout, activation)\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # æ˜¯å¦å…ˆè¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-LNï¼‰è¿˜æ˜¯åå½’ä¸€åŒ–ï¼ˆPost-LNï¼‰\n",
    "        self.norm_first = norm_first\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # æ˜¯å¦å…ˆå½’ä¸€åŒ–å–å†³äºnorm_firstå‚æ•°\n",
    "        if self.norm_first:\n",
    "            # Pre-LN\n",
    "            attn_output = self._sa_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            src = src + attn_output\n",
    "            src = src + self._ff_block(self.norm2(src))\n",
    "        else:\n",
    "            # Post-LN (åŸå§‹Transformeræ¶æ„)\n",
    "            attn_output = self._sa_block(src, src_mask, src_key_padding_mask)\n",
    "            src = self.norm1(src + attn_output)\n",
    "            src = self.norm2(src + self._ff_block(src))\n",
    "        \n",
    "        return src\n",
    "    \n",
    "    # è‡ªæ³¨æ„åŠ›å—\n",
    "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
    "        x, _ = self.self_attn(x, x, x, attn_mask, key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    # å‰é¦ˆç½‘ç»œå—\n",
    "    def _ff_block(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return self.dropout2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ¼å…¬å¼è§£æï¼ˆå®ç°ï¼‰\n",
    "\n",
    "\n",
    "å…¬å¼ä¸­$d_{model}$æŒ‡çš„æ˜¯MLAç±»ä¸­çš„å‚æ•°$dim$ï¼Œå…¬å¼ä¸­çš„$d_k$ æŒ‡çš„æ˜¯MLAç±»ä¸­çš„å‚æ•°$qk\\_head\\_dim$  \n",
    "\n",
    "#### qå‘é‡è®¡ç®—\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "c^Q &= h_t W_{DQ} \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{q\\_lora\\_rank}) \\to (B,L,d_{q\\_lora\\_rank}) \\\\\n",
    "q^C &= c^Q W_{UQ} \\quad (B,L,d_{q\\_lora\\_rank}) \\times (d_{q\\_lora\\_rank}, h \\cdot d_{qk\\_nope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_nope\\_head\\_dim}) \\\\\n",
    "q^R &= \\text{RoPE}(c^Q W_{QR}) \\quad (B,L,d_{q\\_lora\\_rank}) \\times (d_{q\\_lora\\_rank}, h \\cdot d_{qk\\_rope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_rope\\_head\\_dim}) \\\\\n",
    "Q &= \\text{Concat}(q^C, q^R) \\to (B,h,L,d_k), \\quad d_k = d_{qk\\_nope\\_head\\_dim} + d_{qk\\_rope\\_head\\_dim}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "#### kv å‘é‡è®¡ç®—\n",
    "\n",
    "$$\\begin{aligned}\n",
    "c^{KV} &= h_t W_{DKV} \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, d_{kv\\_lora\\_rank}) \\to (B,L,d_{kv\\_lora\\_rank}) \\\\\n",
    "k^C &= c^{KV} W_{UK} \\quad (B,L,d_{kv\\_lora\\_rank}) \\times (d_{kv\\_lora\\_rank}, h \\cdot d_{qk\\_nope\\_head\\_dim}) \\to (B,L,h,d_{qk\\_nope\\_head\\_dim}) \\\\\n",
    "k^R &= \\text{RoPE}(h_t W_{KR}) \\quad (B,L,d_{\\text{model}}) \\times (d_{\\text{model}}, 1 \\cdot d_{qk\\_rope\\_head\\_dim}) \\to (B,L,1,d_{qk\\_rope\\_head\\_dim}) \\to \\text{broadcast} \\to (B,L,h,d_{qk\\_rope\\_head\\_dim}) \\\\\n",
    "K &= \\text{Concat}(k^C, k^R) \\to (B,h,L,d_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "V &= c^{KV} W_{UV} \\quad (B,L,d_{kv\\_lora\\_rank}) \\times (d_{kv\\_lora\\_rank}, h \\cdot d_{v\\_head\\_dim}) \\to (B,L,h,d_{v\\_head\\_dim}) \\to (B,h,L,d_{v\\_head\\_dim})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### self-attention è®¡ç®—\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Attention}(Q,K,V) \n",
    "&= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\\\\n",
    "&= \\text{softmax}\\!\\left(\\frac{Q^C (K^C)^\\top + Q^R (K^R)^\\top}{\\sqrt{d_k}}\\right)V \\\\\n",
    "&\\to (B,h,L,d_k) \\times (B,h,L,d_k)^\\top \\to (B,h,L,L) \\\\\n",
    "&\\to \\text{softmax}(B,h,L,L) \\times (B,h,L,d_{v\\_head\\_dim}) \\to (B,h,L,d_{v\\_head\\_dim})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{MLA}(Q,K,V) &= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O \\\\\n",
    "&\\to (B,h,L,d_{v\\_head\\_dim}) \\to (B,L,h \\cdot d_{v\\_head\\_dim}) \\times (h \\cdot d_{v\\_head\\_dim}, d_{\\text{model}}) \\to (B,L,d_{\\text{model}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "åœ¨è®¡ç®—$q^C (k^C)^\\top$çš„æ—¶å€™ï¼ŒæŒ‰ç…§ä¸‹åˆ—çš„è®¡ç®—æ–¹å¼\n",
    "\n",
    "$q^{C} k^{C^T} = c^Q W_{UQ}(c^{KV} W_{UK})^T = c^Q W_{UQ} (W_{UK})^T (c^{KV})^T =(c^Q W_{UQ} (W_{UK})^T) (c^{KV})^T$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®ç°è¯´æ˜\n",
    "ä»£ç ä¸­ç»™å‡ºäº†ä¸¤ç§è®¡ç®—æ–¹æ³•ï¼Œä¸€ç§æ˜¯naiveï¼Œnaiveå°±æ˜¯ä¸Šé¢çš„å…¬å¼ï¼Œè€Œå¦ä¸€ç§æ˜¯absorbï¼Œåˆ™æ˜¯mlaçš„é€šç”¨å®ç°ï¼Œè®¡ç®—æ•ˆç‡ä»¥åŠKV cacheéƒ½æ›´ä¼˜ï¼Œ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![MLA](asserts/mla.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # è®¡ç®—RMS Norm\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):\n",
    "    \"\"\"é¢„è®¡ç®—ä½ç½®ç¼–ç çš„å¤æ•°è¡¨ç¤º\"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(max_seq_len).float()\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    \"\"\"åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç \"\"\"\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.view(1, x_complex.size(1), 1, x_complex.size(-1))\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs_cis).flatten(3)\n",
    "    return x_rotated.type_as(x)\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    \"\"\"å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMulti-head Latent Attentionï¼‰\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 n_heads, \n",
    "                 qk_nope_head_dim=128,  # éä½ç½®ç¼–ç éƒ¨åˆ†çš„å¤´ç»´åº¦\n",
    "                 qk_rope_head_dim=64,   # ä½ç½®ç¼–ç éƒ¨åˆ†çš„å¤´ç»´åº¦\n",
    "                 v_head_dim=128,        # å€¼çš„å¤´ç»´åº¦\n",
    "                 q_lora_rank=0,         # æŸ¥è¯¢çš„ä½ç§©æŠ•å½±ç»´åº¦\n",
    "                 kv_lora_rank=512,      # é”®å€¼çš„ä½ç§©æŠ•å½±ç»´åº¦\n",
    "                 dropout=0.0,\n",
    "                 attn_impl=\"naive\"):    # æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼šnaiveæˆ–absorb\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.qk_nope_head_dim = qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = qk_rope_head_dim\n",
    "        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n",
    "        self.v_head_dim = v_head_dim\n",
    "        self.q_lora_rank = q_lora_rank\n",
    "        self.kv_lora_rank = kv_lora_rank\n",
    "        self.attn_impl = attn_impl\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›ç¼©æ”¾å› å­\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        \n",
    "        # é’ˆå¯¹æŸ¥è¯¢çš„æŠ•å½±å±‚ - å¯é€‰ä½¿ç”¨ä½ç§©æŠ•å½±\n",
    "        if self.q_lora_rank == 0:\n",
    "            # ç›´æ¥æŠ•å½±\n",
    "            self.wq = nn.Linear(dim, n_heads * self.qk_head_dim)\n",
    "        else:\n",
    "            # ä½¿ç”¨ä½ç§©æŠ•å½± (LoRA)\n",
    "            self.wq_a = nn.Linear(dim, self.q_lora_rank)\n",
    "            self.q_norm = RMSNorm(self.q_lora_rank)\n",
    "            self.wq_b = nn.Linear(self.q_lora_rank, n_heads * self.qk_head_dim)\n",
    "        \n",
    "        # é”®å€¼ä½¿ç”¨ä½ç§©æŠ•å½±\n",
    "        self.wkv_a = nn.Linear(dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        self.wo = nn.Linear(n_heads * self.v_head_dim, dim)\n",
    "        \n",
    "        # dropoutå±‚\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, start_pos, freqs_cis, attention_mask=None, max_seq_len=4096):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        end_pos = start_pos + seq_len\n",
    "        \n",
    "        # ç”Ÿæˆqå‘é‡ - å¯é€‰ä½¿ç”¨ä½ç§©æŠ•å½±\n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        else:\n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x)))\n",
    "        \n",
    "        # å°†qåˆ†ä¸ºéä½ç½®ç¼–ç éƒ¨åˆ†å’Œä½ç½®ç¼–ç éƒ¨åˆ†\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        \n",
    "        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°q_pe\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "        \n",
    "        # ç”Ÿæˆkvå‘é‡ - ä½¿ç”¨ä½ç§©æŠ•å½±\n",
    "        kv = self.wkv_a(x)\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n",
    "        \n",
    "        # ä½¿ç”¨naiveæ–¹å¼å®ç°æ³¨æ„åŠ›\n",
    "        if self.attn_impl == \"naive\":\n",
    "            # å®Œæ•´çš„queryå‘é‡\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1)\n",
    "            \n",
    "            # é€šè¿‡ä½ç§©æŠ•å½±ç”Ÿæˆé”®å€¼\n",
    "            kv = self.wkv_b(self.kv_norm(kv))\n",
    "            kv = kv.view(batch_size, seq_len, self.n_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "            \n",
    "            # å®Œæ•´çš„keyå‘é‡\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_heads, -1)], dim=-1)\n",
    "            \n",
    "            # åˆ›å»ºKVç¼“å­˜\n",
    "            k_cache = torch.zeros(batch_size, max_seq_len, self.n_heads, self.qk_head_dim, device=x.device, dtype=x.dtype)\n",
    "            v_cache = torch.zeros(batch_size, max_seq_len, self.n_heads, self.v_head_dim, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            # æ›´æ–°KVç¼“å­˜\n",
    "            k_cache[:, start_pos:end_pos] = k\n",
    "            v_cache[:, start_pos:end_pos] = v\n",
    "            \n",
    "            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q, k_cache[:, :end_pos]) * self.softmax_scale\n",
    "            \n",
    "        else:  # absorbæ–¹å¼å®ç°æ³¨æ„åŠ›\n",
    "            # è·å–wkv_bæƒé‡\n",
    "            wkv_b = self.wkv_b.weight.view(self.n_heads, -1, self.kv_lora_rank)\n",
    "            \n",
    "            # è®¡ç®—q_nopeä¸æƒé‡çš„ç‚¹ç§¯\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim])\n",
    "            \n",
    "            # åˆ›å»ºKVç¼“å­˜\n",
    "            kv_cache = torch.zeros(batch_size, max_seq_len, self.kv_lora_rank, device=x.device, dtype=x.dtype)\n",
    "            pe_cache = torch.zeros(batch_size, max_seq_len, self.qk_rope_head_dim, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            # æ›´æ–°KVç¼“å­˜\n",
    "            kv_cache[:, start_pos:end_pos] = self.kv_norm(kv)\n",
    "            pe_cache[:, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "            \n",
    "            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° - åˆ†åˆ«è®¡ç®—éä½ç½®ç¼–ç éƒ¨åˆ†å’Œä½ç½®ç¼–ç éƒ¨åˆ†\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, kv_cache[:, :end_pos]) + \n",
    "                     torch.einsum(\"bshr,btr->bsht\", q_pe, pe_cache[:, :end_pos])) * self.softmax_scale\n",
    "        \n",
    "        # åº”ç”¨æ³¨æ„åŠ›æ©ç \n",
    "        if attention_mask is not None:\n",
    "            scores += attention_mask.unsqueeze(1)\n",
    "        \n",
    "        # æ³¨æ„åŠ›æƒé‡è®¡ç®—\n",
    "        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).type_as(x)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # è®¡ç®—è¾“å‡º\n",
    "        if self.attn_impl == \"naive\":\n",
    "            output = torch.einsum(\"bsht,bthd->bshd\", attn_weights, v_cache[:, :end_pos])\n",
    "        else:\n",
    "            # å…ˆä¸kv_cacheç›¸ä¹˜\n",
    "            output = torch.einsum(\"bsht,btc->bshc\", attn_weights, kv_cache[:, :end_pos])\n",
    "            # å†ä¸æƒé‡ç›¸ä¹˜ç”Ÿæˆæœ€ç»ˆè¾“å‡º\n",
    "            output = torch.einsum(\"bshc,hdc->bshd\", output, wkv_b[:, -self.v_head_dim:])\n",
    "        \n",
    "        # é‡å¡‘å¹¶æŠ•å½±åˆ°åŸå§‹ç»´åº¦\n",
    "        output = output.reshape(batch_size, seq_len, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "class MLABlock(nn.Module):\n",
    "    \"\"\"åŒ…å«MLAæ³¨æ„åŠ›æœºåˆ¶çš„Transformerå—\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim=768, \n",
    "                 n_heads=12, \n",
    "                 qk_nope_head_dim=128, \n",
    "                 qk_rope_head_dim=64, \n",
    "                 v_head_dim=128,\n",
    "                 q_lora_rank=0, \n",
    "                 kv_lora_rank=512, \n",
    "                 mlp_ratio=4, \n",
    "                 dropout=0.1, \n",
    "                 attn_impl=\"naive\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # æ³¨æ„åŠ›å±‚\n",
    "        self.attention = MLA(\n",
    "            dim=dim,\n",
    "            n_heads=n_heads,\n",
    "            qk_nope_head_dim=qk_nope_head_dim,\n",
    "            qk_rope_head_dim=qk_rope_head_dim,\n",
    "            v_head_dim=v_head_dim,\n",
    "            q_lora_rank=q_lora_rank,\n",
    "            kv_lora_rank=kv_lora_rank,\n",
    "            dropout=dropout,\n",
    "            attn_impl=attn_impl\n",
    "        )\n",
    "        \n",
    "        # å‰é¦ˆç½‘ç»œ\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        \n",
    "    def forward(self, x, start_pos, freqs_cis, attention_mask=None):\n",
    "        # æ®‹å·®è¿æ¥ + æ³¨æ„åŠ›å±‚\n",
    "        x = x + self.attention(self.norm1(x), start_pos, freqs_cis, attention_mask)\n",
    "        # æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œ\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# ç®€å•ä½¿ç”¨ç¤ºä¾‹\n",
    "def mla_example():\n",
    "    # è®¾ç½®å‚æ•°\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    dim = 512\n",
    "    n_heads = 8\n",
    "    max_seq_len = 4096\n",
    "    \n",
    "    # åˆ›å»ºè¾“å…¥\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # è®¡ç®—ä½ç½®ç¼–ç \n",
    "    freqs_cis = precompute_freqs_cis(64, max_seq_len)\n",
    "    freqs_cis = freqs_cis[:seq_len]\n",
    "    \n",
    "    # åˆ›å»ºæ³¨æ„åŠ›æ©ç  (å› æœæ©ç )\n",
    "    mask = torch.full((seq_len, seq_len), float('-inf')).triu_(1)\n",
    "    \n",
    "    # åˆ›å»ºMLAå—\n",
    "    mla_block = MLABlock(\n",
    "        dim=dim,\n",
    "        n_heads=n_heads,\n",
    "        attn_impl=\"absorb\"  # ä½¿ç”¨absorbå®ç°\n",
    "    )\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    output = mla_block(x, start_pos=0, freqs_cis=freqs_cis, attention_mask=mask)\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    \n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
