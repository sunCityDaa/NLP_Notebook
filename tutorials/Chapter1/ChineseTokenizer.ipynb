{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitpytorchlatestconda37dda3a0837247e597f023e05705e960",
   "display_name": "Python 3.7.3 64-bit ('pytorch_latest': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词\n",
    "与大部分印欧语系的语言不同，中文在词与词之间没有任何空格之类的显示标志指示词的边界。因此，中文分词是很多自然语言处理系统中的基础模块和首要环节。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词基本原理\n",
    "从20世纪80年代或更早的时候起，学者们研究了很多的分词方法，这些方法大致可以分为三大类：\n",
    "\n",
    "基于词表的分词方法\n",
    "- 正向最大匹配法(forward maximum matching method, FMM)\n",
    "- 逆向最大匹配法(backward maximum matching method, BMM)\n",
    "- N-最短路径方法\n",
    "基于统计模型的分词方法\n",
    "- 基于N-gram语言模型的分词方法\n",
    "基于序列标注的分词方法\n",
    "- 基于HMM的分词方法\n",
    "- 基于CRF的分词方法\n",
    "- 基于词感知机的分词方法\n",
    "- 基于深度学习的端到端的分词方法\n",
    "\n",
    "在这里只介绍jieba分词用到的**基于N-gram语言模型的分词方法**和**基于HMM的分词方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 基于N-gram语言模型的分词方法\n",
    "假设随机变量S为一个汉字序列，W是S上所有可能的切分路径。对于分词，实际上就是求解使条件概率P(W∣S)最大的切分路径W∗，即\n",
    "$$\n",
    "W^*=\\mathop{\\arg\\max}\\limits_{W}P(W|S)\n",
    "$$\n",
    "根据贝叶斯公式\n",
    "$$\n",
    "W*=\\mathop{\\arg\\max}\\limits_{W}\\frac{P(W)P(S|W)}{P(S)}\n",
    "$$\n",
    "由于P(S)为归一化因子，P(S∣W)恒为1，因此只需要求解P(W)。P(W)使用N-gram语言模型建模，定义如下(以Bi-gram为例)：\n",
    "$$\n",
    "P(W)=P(w_0,w_1,...w_n)=P(w_0)P(w_1|w_0)P(w_2|w_1)...P(w_n|w_{n-1})=P(w_0)\\prod \\limits_{t=1}^nP(w_n|w_{n-1})\n",
    "$$\n",
    "至此，各切分路径的好坏程度(条件概率P(W∣S))可以求解。简单的，可以根据DAG枚举全路径，暴力求解最优路径；也可以使用动态规划的方法求解，jieba中不带HMM新词发现的分词，就是DAG + Uni-gram的语言模型 + 后向DP的方式进行的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于HMM的分词方法\n",
    "该方法属于由字构词的分词方法，由字构词的分词方法思想并不复杂，它是将分词问题转化为字的分类问题（序列标注问题）。从某些层面讲，由字构词的方法并不依赖于事先编制好的词表，但仍然需要分好词的训练语料。\n",
    "\n",
    "规定每个字有4个词位：\n",
    "- 词首 B\n",
    "- 词中 M\n",
    "- 词尾 E\n",
    "- 单字成词 S\n",
    "\n",
    "![序列标注问题示例](./assets/序列标注问题.png)\n",
    "\n",
    "由于HMM是一个生成式模型，X为观测序列，Y为隐序列。基于HMM的两个假设\n",
    "- 齐次马尔科夫性假设，即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其它时刻的状态及观测无关，也与时刻t无关；\n",
    "- 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其它观测和状态无关，\n",
    "\n",
    "HMM模型中的五元组表示：\n",
    "- 观测序列\n",
    "- 隐藏状态序列\n",
    "- 状态初始概率\n",
    "- 状态转移概率\n",
    "- 状态发射概率\n",
    "\n",
    "最的模型为：\n",
    "$$\n",
    "P(X, Y)=P(y_0)P(y_0|x_0)\\prod \\limits_{t=1}^nP(y_t|y_{t-1})P(x_t|y_t)\n",
    "$$\n",
    "![HMM模型](./assets/HMM模型.png)\n",
    "\n",
    "其中X为观测序列，Y为隐藏状态序列（B，M，E，S）,$P(y_0)$位状态初始概率，$P(y_t|y_{t-1})$为状态转移概率，$P(x_t|y_t)$为状态发射概率。\n",
    "\n",
    "HMM模型有三个基本问题：\n",
    "\n",
    "- 概率计算问题，HMM的五元组，计算在模型下给定隐藏序列Y，计算观测序列X出现的概率也就是Forward-backward算法；\n",
    "\n",
    "- 学习问题，已知观测序列{X}，隐藏序列{Y} ，估计模型的状态初始概率，状态转移概率和状态发射概率 ，使得在该模型下观测序列X的概率尽可能的大，即用极大似然估计的方法估计参数；\n",
    "\n",
    "- 预测问题，也称为解码问题，已知模型状态初始概率，状态转移概率和状态发射概率和观测序列X，求最大概率的隐藏序列Y。\n",
    "\n",
    "其中，jieba分词主要中主要涉及第三个问题，也即预测问题。计算方法会涉及到维特比算法，这个后面会结合代码讲到。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba分词\n",
    "下面我们以jieba分词为例，结合上满介绍的原理和代码介绍一下分词的内部原理，并参考jieba分词源码给出一个简单的实现版本。\n",
    "jieba的分词过程可以概括为以下几个步骤\n",
    "- 依据统计词典（模型中这部分已经具备，也可自定义加载）构建统计词典中词的前缀词典。\n",
    "- 对输入的内容按照子句进行分割（使用正则表达式，以标点符号或者非中文字符为分界）。\n",
    "- 依据前缀词典对输入的句子进行DAG（有向无环图）的构造。\n",
    "- 使用动态规划的方法在DAG上找到一条概率最大路径，依据此路径进行分词。\n",
    "- 对于未收录词（是指不在统计词典中出现的词，未收录词怎么识别可以看完第三部分之后思考一下），使用HMM(隐马尔克夫模型)模型，用Viterbi（维特比）算法找出最可能出现的隐状态序列。\n",
    "\n",
    "![jieba分词算法流程图](./assets/jieba分词算法流程图.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成前缀词典\n",
    "统计词典在jieba包的dict.txt文件中，是开发者已经统计好的词典，第一列代表的是词语，第二列是词频，第三列是词性，我们主要用到前两列信息，词性这部分，这里没有涉及。我们先看一下词典中的部分内容：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "孟玉楼 3 nr\n\n斗蓬 3 ns\n\n铁法官 3 n\n\n羿射九日 3 nr\n\n占金丰 4 nr\n\n"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open(\"assets/dict.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in random.choices(lines, k=5):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当程序运行的时候，它会加载统计词典生成前缀词典，前缀词典是表示什么的呢，我们举个简单的例子。\n",
    "\n",
    "比如统计词典中含有如下词语\n",
    "```\n",
    "我  123\n",
    "在  234\n",
    "学习  456\n",
    "结巴  345\n",
    "分词  456\n",
    "结巴分词  23\n",
    "学  2344\n",
    "分  23\n",
    "结 234\n",
    "```\n",
    "则前缀词典构造如下，它是将在统计词典中出现的每一个词的每一个前缀提取出来，统计词频，如果某个前缀词在统计词典中没有出现，词频统计为0，如果这个前缀词已经统计过，则不再重复。\n",
    "```\n",
    "我 123\n",
    "在  234\n",
    "学  2344\n",
    "学习  456\n",
    "结  234\n",
    "结巴  345\n",
    "结巴分  0\n",
    "结巴分词  23\n",
    "分 23\n",
    "分词  456\n",
    "```\n",
    "这里把未出现的统计词也统计出来，且词频统计为0，是为了后面构造DAG方便。生成前缀词典的代码如下，在jieba分词中前缀词典一般会进行缓存，不需要每次分词都重新加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "生成前缀词典的大小为60102007。\n"
    }
   ],
   "source": [
    "def get_prefix_dict(f_name):\n",
    "    lfreq = {}\n",
    "    ltotal = 0\n",
    "    f = open(f_name)\n",
    "    for lineno, line in enumerate(f, 1):\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            word, freq = line.split(' ')[:2]\n",
    "            freq = int(freq)\n",
    "            lfreq[word] = freq\n",
    "            ltotal += freq\n",
    "            for ch in range(len(word)):\n",
    "                wfrag = word[:ch + 1]\n",
    "                if wfrag not in lfreq:\n",
    "                    lfreq[wfrag] = 0\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n",
    "    f.close()\n",
    "    return lfreq, ltotal\n",
    "\n",
    "freq, total = get_prefix_dict(\"assets/dict.txt\")\n",
    "print(\"生成前缀词典的大小为{}。\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割子句\n",
    "假如我们要对`\"我爱结巴分词。我叫孙悟空，我爱北京，我爱Python和C++。 《机器翻译》这本书是我的最爱。\"`这句话进行分词，我们首先要把它们划分为子句，第一个原因是标点符号是天然的词语间隔，我们的词语中不会包含标点符号。第二个原因是我们的词典中可能没有包含标点符号的内容，我们应当以这些非中文字符、标点字符作为分界，将输入内容划分为子句，对每个子句进行分词。\n",
    "\n",
    "一个可行的实现方法是列举所有在中文词典中可能会出现的字符，将连续出现的这些字符作为一个子句进行划分，这些字符之外的其他符号，我们便可以认为是中文标点符号，并把他们作为子句划分标志。我们可以简单的使用正则表达式来完成，出现在中文词典中的字符可能是中文字符、阿拉伯数字、英文字母、+=.等部分英文数字标点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['', '我爱结巴分词', '。', '我叫孙悟空', '，', '我爱北京', '，', '我爱Python和C++', '。 《', '机器翻译', '》', '这本书是我的最爱', '。']\n"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "example = \"我爱结巴分词。我叫孙悟空，我爱北京，我爱Python和C++。 《机器翻译》这本书是我的最爱。\"\n",
    "\n",
    "# 列举所有中文词中可能包含的字符\n",
    "re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
    "\n",
    "# 将连续出现的合法字符作为一个子句的划分\n",
    "blocks = re_han_default.split(example)\n",
    "\n",
    "print(blocks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到我们已经将整句话分割成子句，每个子句中不再包含标点符号。对于标点符号部分，单独的标点符号我们可以将它直接作为一个单词，而对于`'。 《'`这种情况，我们可以用空白字符\\t\\r\\n将它们进一步分开。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['。', ' ', '《']\n"
    }
   ],
   "source": [
    "re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
    "\n",
    "example = \"。 《\"\n",
    "\n",
    "words = re_skip_default.split(example)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造DAG\n",
    "我们来讲解一下程序里是怎么存储“DAG”的，程序实现图的构建是存储为字典形式的，以每个字所在的位置为键值key，相应划分的末尾位置构成的列表为value，相应划分的末尾位置指的是什么呢，我们举例来说明\n",
    "\n",
    "“我在学习结巴分词”\n",
    "在这句话里，我们将每一个字用所在位置来代表，比如0代表“我”，4代表“结”，针对“结”，我们可以在前缀词典里看到以“结”为前缀的词“结”，“结巴”，“结巴分词”的词频大于0，因此“结”，“巴”，“词”为相应划分的末尾位置，因此键值4的value为[4,5,7]，其他键值的对应value统计如下\n",
    "```\n",
    "0 ：[0]\n",
    "1 ：[1]\n",
    "2 ：[2,3]\n",
    "3 ：[3]\n",
    "4 ：[4,5,7]\n",
    "5 ：[5]\n",
    "6 ：[6,7]\n",
    "7 ：[7]\n",
    "```\n",
    "注：每一个字都将其自己作为相应划分的末尾位置，即使这个字不在统计词典里。\n",
    "\n",
    "基于以上构建的键值对，我们将有向图可视化一下，以便方便理解。\n",
    "![DAG](./assets/DAG.jpg)\n",
    "\n",
    "从“我”到“词”的路径有以下10种\n",
    "```\n",
    "我/在/学/习/结/巴/分/词\n",
    "我/在/学习/结巴分词\n",
    "我/在/学习/结/巴/分/词\n",
    "我/在/学习/结巴/分词\n",
    "我/在/学习/结/巴/分词\n",
    "我/在/学习/结巴/分/词\n",
    "我/在/学/习/结/巴/分词\n",
    "我/在/学/习/结巴/分/词\n",
    "我/在/学/习/结巴分词\n",
    "我/在/学/习/结巴/分词\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{0: [0], 1: [1], 2: [2, 3], 3: [3], 4: [4, 5, 7], 5: [5], 6: [6, 7], 7: [7]}\n"
    }
   ],
   "source": [
    "def get_DAG(sentence, freq):\n",
    "    DAG = {}\n",
    "    N = len(sentence)\n",
    "    for k in range(N):\n",
    "        tmplist = []\n",
    "        i = k\n",
    "        frag = sentence[k]\n",
    "        while i < N and frag in freq:\n",
    "            if freq[frag]:\n",
    "                tmplist.append(i)\n",
    "            i += 1\n",
    "            frag = sentence[k:i + 1]\n",
    "        if not tmplist:\n",
    "            tmplist.append(k)\n",
    "        DAG[k] = tmplist\n",
    "    return DAG\n",
    "\n",
    "example = \"我在学习结巴分词\"\n",
    "dag = get_DAG(example, freq)\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动态规划找到最大路径\n",
    "接下来我们需要计算上面10条路径那一条的可能性最大，将可能性最大的那条路径对应的划分作为我们的分词结果。\n",
    "$$\n",
    "W^*=\\mathop{\\arg\\min}\\limits_{W}P(W)\n",
    "$$\n",
    "其中$W$为句子的一个划分，${w_1,w_2,...wn}$\n",
    "$$\n",
    "P(W)=P(w_1,w_2...wn)=\\prod \\limits_{i=0}^nP(w_n)\n",
    "$$\n",
    "\n",
    "每一个词出现的概率等于该词在前缀里的词频除以所有词的词频之和。如果词频为0或是不存在，当做词频为1来处理。\n",
    "$$\n",
    "P(w_n)=\\frac{freq[w_n]+1}{total}\n",
    "$$\n",
    "\n",
    "这里会取对数概率，即在每个词概率的基础上取对数，一是为了防止下溢，二后面的概率相乘可以变成相加计算。\n",
    "\n",
    "最后我们使用动态规划算法算出概率最大的路径。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{8: (0, 0), 7: (-9.257210763727148, 7), 6: (-14.967114814124178, 7), 5: (-24.384334710144643, 5), 4: (-14.222674339176683, 7), 3: (-25.03090606994119, 3), 2: (-22.62511739105392, 3), 1: (-27.038731622224248, 1), 0: (-32.24695578526084, 0)}\n"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def clac(sentence, DAG, freq, total):\n",
    "    n = len(sentence)\n",
    "    route = {n: (0, 0)}\n",
    "    log_total = log(total)\n",
    "\n",
    "    for i in range(n-1, -1, -1):\n",
    "        cache = []\n",
    "        for j in DAG[i]:\n",
    "            log_p = log(freq.get(sentence[i:j+1], 0) or 1)\n",
    "            cache.append((log_p - log_total + route[j+1][0], j))\n",
    "        route[i] = max(cache)\n",
    "    return route\n",
    "\n",
    "route = clac(example, dag, freq, total)\n",
    "print(route)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的计算结果，`route`中的key代表最优路径中当前词的起始位置，value的第二个元素代表最优路径中当前词的末尾位置，通过这两个量我们可以推出一个初步的基于词典和词频的分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "我\n在\n学习\n结巴分词\n"
    }
   ],
   "source": [
    "def cut_no_hmm(sentence, route):\n",
    "    i = 0\n",
    "    while(i < len(route)-1):\n",
    "        j = route[i][1]\n",
    "        yield sentence[i:j+1]\n",
    "        i = j + 1\n",
    "\n",
    "for word in cut_no_hmm(example, route):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM算法对于未登录词的识别\n",
    "在jieba分词中，基于HMM的分词主要是作为基于Uni—gram分词的一个补充，主要是解决OOV（out of vocabulary）问题的，它的作用是对未登录词典的词进行识别发现。我们首先用一个例子说明HMM的重要性。比如我们要对一个包含人名的句子进行分词，“韩冰是个好人”。“韩冰”这个词不在词典之中，所以前面基于词典+Uni-Gram语言模型的方法进行分词就会将“韩冰”这个人名分成“韩”+“冰”。所以我们需要一个有一定泛化能力的机器学习模型对这些新词进行发现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['韩', '冰', '是', '个', '好人']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "example = \"韩冰是个好人\"\n",
    "dag = get_DAG(example, freq)\n",
    "route = clac(example, dag, freq, total)\n",
    "\n",
    "list(cut_no_hmm(example, route))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用HMM进行分词的原理在前面已经介绍过了。利用HMM模型进行分词，主要是将分词问题视为一个序列标注（sequence labeling）问题，其中，句子为观测序列，分词结果为状态序列。首先通过语料训练出HMM相关的模型，然后利用Viterbi算法进行求解，最终得到最优的状态序列，然后再根据状态序列，输出分词结果。\n",
    "\n",
    "这里的状态序列的元素有四种\n",
    "- \"B\":Begin（这个字处于词的开始位置）\n",
    "- \"M\":Middle（这个字处于词的中间位置）\n",
    "- \"E\":End（这个字处于词的结束位置）\n",
    "- \"S\":Single（这个字是单字成词）}\n",
    "\n",
    "由于分词算法术语HMM的预测问题（已知模型状态初始概率，状态转移概率和状态发射概率和观测序列X，求最大概率的隐藏序列Y），所以我们需要在已经进行标注的数据集上训练我们模型的参数，也就是初始概率，状态转移概率和状态发射概率。这里jieba分词中包含了一个已经训练好的模型，至于模型数据来源和训练方法，这里不再赘述，可以参考[模型的数据是如何生成的？]https://github.com/fxsjy/jieba/issues/7 。这里我们直接将模型加载进来用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "\n",
    "prob_start = pickle.load(open(\"./assets/prob_start.p\", \"rb\"))  # 初始概率参数\n",
    "prob_emit = pickle.load(open(\"./assets/prob_emit.p\", \"rb\"))  # 发射概率\n",
    "prob_trans = pickle.load(open(\"./assets/prob_trans.p\", \"rb\"))  # 状态转移概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是初始概率，及输入观察序列（带分词句子）首个字符是\"B\",\"M\", \"E\", \"S\"的概率（这里的概率也进行了对数运算及log(p)）。由这个概率值可以看出，句子首字单字成词（S）和作为词的词首（B）的概率较高，作为词中和词尾概率为0，也比较符合我们的常识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'B': 0.7689828525554734, 'E': 0.0, 'M': 0.0, 'S': 0.2310171474445266}"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 为了直观，将log概率转化为真实概率\n",
    "{key:math.exp(value) for key, value in prob_start.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是状态转移概率，及\"B\",\"M\", \"E\", \"S\"四个状态之间相互转化的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'B': {'E': 0.6000000000000004, 'M': 0.4},\n 'E': {'B': 0.5544853051164425, 'S': 0.44551469488355755},\n 'M': {'E': 0.7164487459986911, 'M': 0.2835512540013088},\n 'S': {'B': 0.48617017333894563, 'S': 0.5138298266610544}}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "{key: {k: math.exp(v) for k, v in value.items()} for key, value in prob_trans.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后是发射概率，即在观测序列是某个字的情况下，被标注为\"B\",\"M\", \"E\", \"S\"的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'B': {'一': 0.025874486447195644,\n  '丁': 0.0002960323136559398,\n  '七': 0.0004026703175442123,\n  '万': 0.0018186831560606151,\n  '丈': 0.00014100868588615948},\n 'E': {'一': 0.002369710374262949,\n  '丁': 0.000114401037236071,\n  '七': 0.00010115647270757471,\n  '万': 0.00047351540431744344,\n  '丈': 0.00012050479628052327},\n 'M': {'一': 0.01193645010412285,\n  '丁': 0.00035872815397116633,\n  '七': 0.001416288550382968,\n  '万': 0.0021550909026310924,\n  '丈': 8.165936412282943e-05},\n 'S': {'∶': 1.3353987946490163e-07,\n  '一': 0.007272247985959882,\n  '丁': 0.00012041958630747509,\n  '丂': 6.67699397324508e-08,\n  '七': 0.00025622964372327994}}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 由于这个表比较大，所以随机挑选一些出来看\n",
    "{key: {k: math.exp(v) for i, (k, v) in enumerate(value.items()) if i < 5} for key, value in prob_emit.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了模型，接下来就可以用viterbi算法对给定的序列进行分词。还拿上面的例子举例 \"韩冰是个好人\" -> \\['韩', '冰', '是', '个', '好人'\\]，对于已经成词的部分“好人”，我们不需要对它进行计算了，我们只需要将还是单个字的序列“韩冰是个”放入到HMM模型中进行分词，也就是将这四个字分别打上 “BEMS”标签。并且我们期望的标签是\\['韩'->B, '冰'->M, '是'->S, '个'->S\\]。首先我们简单介绍一下维特比算法。\n",
    "\n",
    "#### Viterbi算法\n",
    "viterbi维特比算法解决的是篱笆型的图的最短路径问题，图的节点按列组织，每列的节点数量可以不一样，每一列的节点只能和相邻列的节点相连，不能跨列相连，节点之间有着不同的距离，距离的值就不在图上一一标注出来了，大家自行脑补。\n",
    "![Viterbi算法](./assets/HMM分词篱笆型图.drawio.png)\n",
    "\n",
    "过程非常简单：\n",
    "\n",
    "为了找出Start到End之间的最短路径，我们先从Start开始从左到右一列一列地来看。首先起点是Start，从Start到“韩”字对应的状态列的路径有四种可能：Start-B、Start-E、Start-M，Start-S。对应的路径长度即\n",
    "\n",
    "![viterbi_step1](./assets/viterbi_step1.drawio.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-8.093263409081425 -3.14e+100 -3.14e+100 -10.534873750321356\n"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "MIN_FLOAT = -3.14e100\n",
    "start_2_B = prob_emit[\"B\"].get(\"韩\", MIN_FLOAT) + prob_start[\"B\"]\n",
    "start_2_E = prob_emit[\"E\"].get(\"韩\", MIN_FLOAT) + prob_start[\"E\"]\n",
    "start_2_M = prob_emit[\"M\"].get(\"韩\", MIN_FLOAT) + prob_start[\"M\"]\n",
    "start_2_S = prob_emit[\"S\"].get(\"韩\", MIN_FLOAT) + prob_start[\"S\"]\n",
    "\n",
    "print(start_2_B, start_2_E, start_2_M, start_2_S)"
   ]
  },
  {
   "source": [
    "我们不能武断地说这四条路径中中的哪一段必定是全局最短路径中的一部分，目前为止任何一段都有可能是全局最优路径的备选项。我们继续往右看，到了“冰”这一列列。按照四个状态进行逐一分析，先看到达“冰”(B)节点的各个路径长度。\n",
    "\n",
    "![viterbi_step2](./assets/viterbi_step2.drawio.png)\n",
    "\n",
    "以上这四条路径，各节点距离加起来对比一下，我们就可以知道其中哪一条是最短的。因为Start-B-B是最短的，那么我们就知道了经过“冰”(B)的所有路径当中Start-B-B是最短的，其它三条路径路径都比Start-B-B长，绝对不是目标答案，可以大胆地删掉了。删掉了不可能是答案的路径，就是viterbi算法（维特比算法）的重点，因为后面我们再也不用考虑这些被删掉的路径了。现在经过“冰”(B)的所有路径只剩一条路径了(红色标识)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-3.14e+100 -3.14e+100 -6.28e+100 -19.68864099798377\n"
    }
   ],
   "source": [
    "B_2_B = start_2_B + prob_trans[\"B\"].get(\"B\", MIN_FLOAT) + prob_emit[\"B\"].get(\"冰\", MIN_FLOAT)\n",
    "E_2_B = start_2_E + prob_trans[\"E\"].get(\"B\", MIN_FLOAT) + prob_emit[\"B\"].get(\"冰\", MIN_FLOAT)\n",
    "M_2_B = start_2_M + prob_trans[\"M\"].get(\"B\", MIN_FLOAT) + prob_emit[\"B\"].get(\"冰\", MIN_FLOAT)\n",
    "S_2_B = start_2_S + prob_trans[\"S\"].get(\"B\", MIN_FLOAT) + prob_emit[\"B\"].get(\"冰\", MIN_FLOAT)\n",
    "\n",
    "print(B_2_B, E_2_B, M_2_B, S_2_B)"
   ]
  },
  {
   "source": [
    "以此类推，我们可以分别找出到达“冰”字对应列的所有四个状态的最优路径。\n",
    "\n",
    "![viterbi_step3](./assets/viterbi_step3.drawio.png)\n",
    "\n",
    "对后面的“是”，“个”也进行同样的操作，我们便可以得到一条全局最优路径。\n",
    "![viterbi_step4](./assets/viterbi_step4.drawio.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "韩 -> B\n冰 -> E\n是 -> S\n个 -> S\n"
    }
   ],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]  # tabular\n",
    "    path = {}\n",
    "    for y in states:  # init\n",
    "        V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)\n",
    "        path[y] = [y]\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "        for y in states:\n",
    "            em_p = emit_p[y].get(obs[t], MIN_FLOAT)\n",
    "            (prob, state) = max(\n",
    "                [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in states])\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[state] + [y]\n",
    "        path = newpath\n",
    "\n",
    "    (prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')\n",
    "\n",
    "    return (prob, path[state])\n",
    "\n",
    "example = \"韩冰是个\"\n",
    "prob, path = viterbi(example, \"BEMS\", prob_start, prob_trans, prob_emit)\n",
    "\n",
    "for w, s in zip(example, path):\n",
    "    print(w, \"->\", s)"
   ]
  },
  {
   "source": [
    "根据HMM输出的结果，我们可以将”韩“->B，”冰“->E合并成为一个新词”韩冰“。所以”韩冰是个好人“的分词结果就是['韩冰', '是', '个', '好人']"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "韩冰\n是\n个\n好人\n"
    }
   ],
   "source": [
    "def hmm(sentence, start_P, trans_P, emit_P):\n",
    "    prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P)\n",
    "    begin, nexti = 0, 0\n",
    "    # print pos_list, sentence\n",
    "    for i, char in enumerate(sentence):\n",
    "        pos = pos_list[i]\n",
    "        if pos == 'B':\n",
    "            begin = i\n",
    "        elif pos == 'E':\n",
    "            yield sentence[begin:i + 1]\n",
    "            nexti = i + 1\n",
    "        elif pos == 'S':\n",
    "            yield char\n",
    "            nexti = i + 1\n",
    "    if nexti < len(sentence):\n",
    "        yield sentence[nexti:]\n",
    "\n",
    "def cut_hmm(sentence):\n",
    "    dag = get_DAG(sentence, freq)\n",
    "    route = clac(sentence, dag, freq, total)\n",
    "    i = 0\n",
    "    buf = \"\"\n",
    "    while(i < len(route)-1):\n",
    "        j = route[i][1] + 1\n",
    "\n",
    "        if j - i <= 1:\n",
    "            buf += sentence[i]\n",
    "        else:\n",
    "            if buf:\n",
    "                if len(buf) == 1:\n",
    "                    yield buf\n",
    "                else:\n",
    "                    if buf not in freq:\n",
    "                        for w in hmm(buf, prob_start, prob_trans, prob_emit):\n",
    "                            yield w\n",
    "                    else:\n",
    "                        for w in buf:\n",
    "                            yield w\n",
    "                buf = \"\"\n",
    "            yield sentence[i:j]\n",
    "        i = j\n",
    "    \n",
    "    if buf:\n",
    "        if len(buf) == 1:\n",
    "            yield buf\n",
    "            buf = \"\"\n",
    "        else:\n",
    "            if buf not in freq:\n",
    "                for w in hmm(buf, prob_start, prob_trans, prob_emit):\n",
    "                    yield w\n",
    "            else:\n",
    "                for w in buf:\n",
    "                    yield w\n",
    "\n",
    "example = \"韩冰是个好人\"\n",
    "for word in cut_hmm(example):\n",
    "    print(word)"
   ]
  },
  {
   "source": [
    "### 正则表达式辅助分词\n",
    "除了上述使用机器学习的方法进行分词之外，在我们翻译语料的分词过程中，经常会遇到一些特殊情况，比如日期、数字、英文单词或者其他符合某个特定规则的词语，在前面的操作中，我们将他们划分到了子句之中，因为词典中某些词也会出现这些字符。但是，对于未出现在词典中的英文、数字、符号组合，我们也希望强制把它们当做一个词进行处理，而不是将它们分开。它们通常很难添加到词典中（因为数字字母的排列组合往往是很大的），却很容易通过一些简单的正则表达式对他们进行处理。\n",
    "```py\n",
    "1920.2333  # 浮点数\n",
    "2020.9.2  # 日期\n",
    "apple  # 英文词\n",
    "```\n",
    "我们来看看如果只用 词典+HMM的方式处理他们会怎么样"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['最终', '结果', '为', '1', '9', '2', '0', '.', '2', '3', '3', '3']\n['今天', '是', '2', '0', '2', '0', '.', '9', '.', '2']\n['A', 'p', 'p', 'l', 'e', '手机', '是', '我', '的', '最', '爱']\n"
    }
   ],
   "source": [
    "sentences = [\"最终结果为1920.2333\", \"今天是2020.9.2\", \"Apple手机是我的最爱\"]\n",
    "for s in sentences:\n",
    "    print(list(cut_hmm(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了处理这个问题，我们需要把连续的、不在词典中的非汉字字符提取出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['最终', '结果', '为', '1920.2333']\n['今天', '是', '2020.9.2']\n['Apple', '手机', '是', '我', '的', '最', '爱']\n"
    }
   ],
   "source": [
    "# 用于提取连续的汉字部分\n",
    "re_han = re.compile(\"([\\u4E00-\\u9FD5]+)\")\n",
    "# 用于分割连续的非汉字部分\n",
    "re_skip = re.compile(\"([a-zA-Z0-9\\.]+(?:\\.\\d+)?%?)\")\n",
    "\n",
    "def cut_regx_hmm(sentence):\n",
    "    blocks = re_han.split(sentence)\n",
    "    for block in blocks:\n",
    "        if not block:\n",
    "            continue\n",
    "        if re_han.match(block):\n",
    "            yield from cut_hmm(block)\n",
    "        else:\n",
    "            for ss in re_skip.split(block):\n",
    "                if ss:\n",
    "                    yield ss\n",
    "\n",
    "for s in sentences:\n",
    "    print(list(cut_regx_hmm(s)))  \n"
   ]
  },
  {
   "source": [
    "## Putting them together.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['程序员',\n '祝',\n '海林',\n '和',\n '朱会震',\n '是',\n '在',\n '孙健',\n '的',\n '左面',\n '和',\n '右面',\n ',',\n ' ',\n '范凯',\n '在',\n '最',\n '右面',\n '。',\n '再往',\n '左',\n '是',\n '李松洪']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "from math import log\n",
    "\n",
    "class ChineseTokenizer(object):\n",
    "\n",
    "    re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
    "    re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
    "\n",
    "    # 用于提取连续的汉字部分\n",
    "    re_han = re.compile(\"([\\u4E00-\\u9FD5]+)\")\n",
    "    # 用于分割连续的非汉字部分\n",
    "    re_skip = re.compile(\"([a-zA-Z0-9\\.]+(?:\\.\\d+)?%?)\")\n",
    "\n",
    "    MIN_FLOAT = -3.14e100\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prefix_dict(f_name):\n",
    "        lfreq = {}\n",
    "        ltotal = 0\n",
    "        f = open(f_name)\n",
    "        for lineno, line in enumerate(f, 1):\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                word, freq = line.split(' ')[:2]\n",
    "                freq = int(freq)\n",
    "                lfreq[word] = freq\n",
    "                ltotal += freq\n",
    "                for ch in range(len(word)):\n",
    "                    wfrag = word[:ch + 1]\n",
    "                    if wfrag not in lfreq:\n",
    "                        lfreq[wfrag] = 0\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n",
    "        f.close()\n",
    "        return lfreq, ltotal\n",
    "\n",
    "    def __init__(self):\n",
    "        self.freq, self.total = self.get_prefix_dict(\"./assets/dict.txt\")  # 前缀词典 \n",
    "        self.prob_start = pickle.load(open(\"./assets/prob_start.p\", \"rb\"))  # 初始概率参数\n",
    "        self.prob_emit = pickle.load(open(\"./assets/prob_emit.p\", \"rb\"))  # 发射概率\n",
    "        self.prob_trans = pickle.load(open(\"./assets/prob_trans.p\", \"rb\"))  # 状态转移概率\n",
    "\n",
    "    def cut(self, sentence):\n",
    "        blocks = self.re_han_default.split(sentence)\n",
    "        for blk in blocks:\n",
    "            # 处理空字符串\n",
    "            if not blk:\n",
    "                continue\n",
    "            if self.re_han_default.match(blk):\n",
    "                # 处理子句\n",
    "                for word in self.cut_block(blk):\n",
    "                    yield word\n",
    "            else:\n",
    "                # 处理标点符号、空格等等\n",
    "                tmp = self.re_skip_default.split(blk)\n",
    "                for x in tmp:\n",
    "                    if self.re_skip_default.match(x):\n",
    "                        # 空格、制表符、换行等一起返回\n",
    "                        yield x\n",
    "                    else:\n",
    "                        # 标点符号等分割成字符返回\n",
    "                        for xx in x:\n",
    "                            yield xx\n",
    "\n",
    "\n",
    "    def cut_block(self, sentence):\n",
    "        DAG = self.get_DAG(sentence)\n",
    "        route = self.clac(sentence, DAG)\n",
    "        x = 0\n",
    "        buf = ''\n",
    "        N = len(sentence)\n",
    "        while x < N:\n",
    "            y = route[x][1] + 1\n",
    "            l_word = sentence[x:y]\n",
    "\n",
    "            # 如果当前为一个字符，加入buffer待HMM进一步分词\n",
    "            if y - x == 1:\n",
    "                buf += l_word\n",
    "            else:\n",
    "                # 对当前buffer进行分词\n",
    "                if buf:\n",
    "                    # 当前buffer只有一个字符，直接yield\n",
    "                    if len(buf) == 1:\n",
    "                        yield buf\n",
    "                        buf = ''\n",
    "                    else:\n",
    "                        # 这里加了一层判断，如果词典中存在和当前buffer相同的词，则不需要再用HMM进行切分了。\n",
    "                        if not self.freq.get(buf):\n",
    "                            # 讲buffer送入HMM进行分词\n",
    "                            recognized = self.cut_regx_hmm(buf)\n",
    "                            for t in recognized:\n",
    "                                yield t\n",
    "                        else:\n",
    "                            for elem in buf:\n",
    "                                yield elem\n",
    "                        buf = ''\n",
    "                yield l_word\n",
    "            x = y\n",
    "\n",
    "        # 跳出循环后，可能还有待处理的buffer，进行处理\n",
    "        if buf:\n",
    "            if len(buf) == 1:\n",
    "                yield buf\n",
    "            elif not self.freq.get(buf):\n",
    "                recognized = self.cut_regx_hmm(buf)\n",
    "                for t in recognized:\n",
    "                    yield t\n",
    "            else:\n",
    "                for elem in buf:\n",
    "                    yield elem\n",
    "    \n",
    "    def cut_regx_hmm(self, sentence):\n",
    "        blocks = self.re_han.split(sentence)\n",
    "        for block in blocks:\n",
    "            if not block:\n",
    "                continue\n",
    "            if self.re_han.match(block):\n",
    "                yield from self.cut_hmm(block)\n",
    "            else:\n",
    "                for ss in self.re_skip.split(block):\n",
    "                    if ss:\n",
    "                        yield ss\n",
    "\n",
    "    def cut_hmm(self, sentence):\n",
    "        prob, pos_list = self.viterbi(sentence, 'BMES')\n",
    "        begin, nexti = 0, 0\n",
    "        # print pos_list, sentence\n",
    "        for i, char in enumerate(sentence):\n",
    "            pos = pos_list[i]\n",
    "            if pos == 'B':\n",
    "                begin = i\n",
    "            elif pos == 'E':\n",
    "                yield sentence[begin:i + 1]\n",
    "                nexti = i + 1\n",
    "            elif pos == 'S':\n",
    "                yield char\n",
    "                nexti = i + 1\n",
    "        if nexti < len(sentence):\n",
    "            yield sentence[nexti:]\n",
    "\n",
    "    def viterbi(self, obs, states):\n",
    "        V = [{}]  # tabular\n",
    "        path = {}\n",
    "        for y in states:  # init\n",
    "            V[0][y] = self.prob_start[y] + self.prob_emit[y].get(obs[0], self.MIN_FLOAT)\n",
    "            path[y] = [y]\n",
    "        for t in range(1, len(obs)):\n",
    "            V.append({})\n",
    "            newpath = {}\n",
    "            for y in states:\n",
    "                em_p = self.prob_emit[y].get(obs[t], self.MIN_FLOAT)\n",
    "                (prob, state) = max(\n",
    "                    [(V[t - 1][y0] + self.prob_trans[y0].get(y, self.MIN_FLOAT) + em_p, y0) for y0 in states])\n",
    "                V[t][y] = prob\n",
    "                newpath[y] = path[state] + [y]\n",
    "            path = newpath\n",
    "\n",
    "        (prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')\n",
    "\n",
    "        return (prob, path[state])\n",
    "\n",
    "    def get_DAG(self, sentence):\n",
    "        DAG = {}\n",
    "        N = len(sentence)\n",
    "        for k in range(N):\n",
    "            tmplist = []\n",
    "            i = k\n",
    "            frag = sentence[k]\n",
    "            while i < N and frag in self.freq:\n",
    "                if self.freq[frag]:\n",
    "                    tmplist.append(i)\n",
    "                i += 1\n",
    "                frag = sentence[k:i + 1]\n",
    "            if not tmplist:\n",
    "                tmplist.append(k)\n",
    "            DAG[k] = tmplist\n",
    "        return DAG\n",
    "\n",
    "    def clac(self, sentence, DAG):\n",
    "        n = len(sentence)\n",
    "        route = {n: (0, 0)}\n",
    "        log_total = log(self.total)\n",
    "\n",
    "        for i in range(n-1, -1, -1):\n",
    "            cache = []\n",
    "            for j in DAG[i]:\n",
    "                log_p = log(self.freq.get(sentence[i:j+1], 0) or 1)\n",
    "                cache.append((log_p - log_total + route[j+1][0], j))\n",
    "            route[i] = max(cache)\n",
    "        return route    \n",
    "\n",
    "\n",
    "sentence1 = \"程序员祝海林和朱会震是在孙健的左面和右面, 范凯在最右面。再往左是李松洪\"\n",
    "tokenizer = ChineseTokenizer()\n",
    "list(tokenizer.cut(sentence1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [Github: jieba](https://github.com/fxsjy/jieba)\n",
    "- [Github: sacremoses](https://github.com/alvations/sacremoses)\n",
    "- [知乎：jieba分词的原理](https://zhuanlan.zhihu.com/p/189410443)\n",
    "\n"
   ]
  }
 ]
}