{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文分词\n",
    "分词是数据预处理的第一步。。对于像中文这样没有单词边界的语言，分词的策略通常比较复杂。现在常用的一些中文分词工具有 NLTK、jieba等。而像英文这种有单词边界的语言，分词要简单许多，比如，Moses 工具包就有可以处理绝大多数拉丁语系语言的分词脚本。\n",
    "\n",
    "本章节就以[sacremoses](https://github.com/alvations/sacremoses)为例，讲解英文的分词过程。\n",
    "\n",
    "目录：\n",
    "1. 替换空白字符\n",
    "2. 去掉句子开头和结尾的空白字符\n",
    "3. 将常见标点、乱码等符号与词语分开\n",
    "4. 分割逗号`,\n",
    "5. 分割句号`,`\n",
    "6. 处理`'`号缩写\n",
    "7. 可选处理项\n",
    "    - Mask受保护字符串\n",
    "    - 分割破折号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 替换空白字符\n",
    "包括空格、换行、tab缩进等所有的空字符，在正则表达式中，我们可以使用`\"\\s+\"`进行匹配。除此之外，在ASCII码中，第0～31号及第127号(共33个)是控制字符或通讯专用字符，如控制符：LF（换行）、CR（回车）、FF（换页）、DEL（删除）、BS（退格)、BEL（振铃）等；通讯专用字符：SOH（文头）、EOT（文尾）、ACK（确认）等，我们可以使用`\"[\\000-\\037]\"`进行匹配。\n",
    "\n",
    "有了对应的正则表达式，在python中我们可以使用`re.sub`函数进行替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a test sentence  \t with useless blank chars\r",
      ".\u0001\n",
      " This is a test sentence with useless blank chars .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "DEDUPLICATE_SPACE = r\"\\s+\", r\" \"\n",
    "ASCII_JUNK = r\"[\\000-\\037]\", r\"\" \n",
    "\n",
    "text = u\" This is a test sentence  \\t with useless blank chars\\r.\\x01\"\n",
    "print(text)\n",
    "\n",
    "for regexp, substitution in [DEDUPLICATE_SPACE, ASCII_JUNK]:\n",
    "    text = re.sub(regexp, substitution, text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去掉句子开头和结尾的空白字符\n",
    "刚才将所有的空白字符替换为了空格，但是句子开头和结尾的空白字符也被替换成了空格，还没有被去掉，所以这里我们使用`strip()`方法去掉开头和结尾的空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test sentence with useless blank chars .\n"
     ]
    }
   ],
   "source": [
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将常见标点、乱码等符号与词语分开\n",
    "在Unicode字符集中，一部分字符会在我们的单词中出现，一部分则为标点符号以及其他的一些符号、乱码，如果我们的平行语料中这些字符通常与我们的单词连在一起，我们需要将它们与正常的单词分开。一个可行的方法是列举出所有可能出现在单词中字符（包括正常标点符号），除此之外的字符都在其两侧添加空格符号。幸运的是，moses中已经为我们搜集了这些字符，我们可以直接拿过来用（注意这里的标点符号不包含`.`，`.`，后续会单独处理）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This,', 'is', 'a', 'sentence', 'with', 'weird', '»', 'symbols', '…', 'appearing', 'everywhere', '¿', '.', 'Olso', 'some', 'normal', 'punctuation', 'such', 'as', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./assets/IsAlnum.txt\") as f:\n",
    "    IsAlnum = f.read()\n",
    "    \n",
    "PAD_NOT_ISALNUM = r\"([^{}\\s\\.'\\`\\,\\-])\".format(IsAlnum), r\" \\1 \"\n",
    "regxp, substitution = PAD_NOT_ISALNUM\n",
    "\n",
    "text = \"This, is a sentence with weird\\xbb symbols\\u2026 appearing everywhere\\xbf. Olso some normal punctuation such as?\"\n",
    "\n",
    "text = re.sub(regxp, substitution, text)\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割逗号`,`\n",
    "这是刚才的遗留问题，由于`,`在用作子句划分的时候（I'm fine, thank you.）我们希望它与前后的单词分开，在某些情况下（如数字5,300）我们不希望将`,`与前后的字符分开。所以我们将`,`进行单独处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'This', 'is', 'a', 'sentence', '10', ',', 'with', 'number', '5,300', ',', 'and', '5', ',']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./assets/IsN.txt\") as f:\n",
    "    IsN = f.read()\n",
    "\n",
    "COMMA_SEPARATE_1 = r\"([^{}])[,]\".format(IsN), r\"\\1 , \"  # 若逗号前面不是数字，则分离逗号，如 hello,120 -> hello , 120\n",
    "COMMA_SEPARATE_2 = r\"[,]([^{}])\".format(IsN), r\" , \\1\"  # 若逗号后面不是数字，则分离逗号，如 120, hello -> 120 , hello\n",
    "COMMA_SEPARATE_3 = r\"([{}])[,]$\".format(IsN), r\"\\1 , \"  # 如果数字后匹配到结尾符，则分离逗号。 如120， -> 120 ,\n",
    "COMMA_SEPARATE_4 = r\"^[,]([{}])\".format(IsN), r\" \\1, \"  # 如果数字后匹配到结尾符，则分离逗号。 如120， -> 120 ,\n",
    "\n",
    "text = \",This is a sentence 10, with number 5,300, and 5,\"\n",
    "\n",
    "# 此版本的实现可能会在此处创建额外的空格，但稍后会删除这些空格\n",
    "for regxp, substitution in [COMMA_SEPARATE_1, COMMA_SEPARATE_2, COMMA_SEPARATE_3, COMMA_SEPARATE_4]:\n",
    "    text = re.sub(regxp, substitution, text)\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割句号`.`\n",
    "与`,`一样，`.`同样需要特殊的规则进行分割。需要考虑的情况有以下几种\n",
    "1. 连续多个点号的情况（省略号）`.....`\n",
    "2. 一个单独的大写字母跟一个`.` (通常出现在人名中，如`Aaron C. Courville`)\n",
    "3. 其他的多字母人名，地名、机构名等缩写。 （如`Gov.`表示政府，Mr.代表某某先生）\n",
    "4. 其他带`.`的缩写。（如`e.g.`表示举例，`i.e.`表示换句话说，`rev.`表示revision）\n",
    "5. 一些`.`后面跟数字的情况（如`No`. `Nos.`），这种情况与前面的区别是只有当这些词后面跟随数字时才不是句子的结束，如`No.`也可能做否定的意思。\n",
    "6. 月份的缩写。（如`Jan.` 表示一月，`Feb.`表示2月）\n",
    "\n",
    "对于情况1，我们先匹配到多个`.`连续出现的情况，在其前后添加空格，并用Mask做标记（防止处理其他情况时对其造成不可预见的影响），待处理完其他情况后再将其还原。\n",
    "\n",
    "对于后面几种情况，我们针对每一中情况建立一个前缀词表，如果`.`前面是这些词的话，就不讲`.`和前面的词分开。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', 'write', 'on', 'Sep.', '6th', '.', 'No', '.', 'I', 'am', '123No.', 'in', 'all', 'people', '.', 'We', 'are', 'good', 'at', 'talk,', 'swiming', 'DOTDOTDOTMULTI']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./assets/nonbreaking_prefix.en\") as f:\n",
    "    NONBREAKING_PREFIXES  = []\n",
    "    NUMERIC_ONLY_PREFIXES = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            if line.endswith(\"#NUMERIC_ONLY#\"):\n",
    "                NUMERIC_ONLY_PREFIXES.append(line.split()[0])\n",
    "            if line not in NONBREAKING_PREFIXES:\n",
    "                NONBREAKING_PREFIXES.append(line)\n",
    "\n",
    "with open(\"./assets/IsAlpha.txt\") as f:\n",
    "    # IsAlnum = IsAlpha + IsN\n",
    "    IsAlpha = f.read()\n",
    "    \n",
    "with open(\"./assets/IsLower.txt\") as f:\n",
    "    IsLower = f.read()\n",
    "\n",
    "def isanyalpha(text):\n",
    "    # 判断给定字符串中是否全是字母（非数字、符号）\n",
    "    return any(set(text).intersection(set(IsAlpha)))\n",
    "\n",
    "def islower(text):\n",
    "    # 判断给定字符串中是否全部都是小写字母\n",
    "    return not set(text).difference(set(IsLower))\n",
    "\n",
    "\n",
    "def replace_multidots(text):\n",
    "    # 处理情况1，对多个\".\"的情况作mask处理\n",
    "    text = re.sub(r\"\\.([\\.]+)\", r\" DOTMULTI\\1\", text)\n",
    "    while re.search(r\"DOTMULTI\\.\", text):\n",
    "        text = re.sub(r\"DOTMULTI\\.([^\\.])\", r\"DOTDOTMULTI \\1\", text)\n",
    "        text = re.sub(r\"DOTMULTI\\.\", \"DOTDOTMULTI\", text)\n",
    "    return text\n",
    "\n",
    "def handles_nonbreaking_prefixes(text):\n",
    "    # 将文本拆分为标记以检查 \".\" 为结尾的部分是否符合拆分条件\n",
    "    tokens = text.split()\n",
    "    num_tokens = len(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 判断是否以\".\"结尾\n",
    "        token_ends_with_period = re.search(r\"^(\\S+)\\.$\", token)\n",
    "        if token_ends_with_period:  \n",
    "            prefix = token_ends_with_period.group(1)\n",
    "\n",
    "            # 处理情况2,3,4,6\n",
    "            if (\n",
    "                (\".\" in prefix and isanyalpha(prefix))\n",
    "                or (\n",
    "                    prefix in NONBREAKING_PREFIXES\n",
    "                    and prefix not in NUMERIC_ONLY_PREFIXES\n",
    "                )\n",
    "                or (\n",
    "                    i != num_tokens - 1\n",
    "                    and tokens[i + 1]\n",
    "                    and islower(tokens[i + 1][0])\n",
    "                )\n",
    "            ):\n",
    "                pass  # 不做拆分处理\n",
    "\n",
    "            # 处理情况 5\n",
    "            elif (\n",
    "                prefix in NUMERIC_ONLY_PREFIXES\n",
    "                and (i + 1) < num_tokens\n",
    "                and re.search(r\"^[0-9]+\", tokens[i + 1])\n",
    "            ):\n",
    "                pass  # 不做拆分处理\n",
    "            else:  # 不在1-6中，做拆分处理\n",
    "                tokens[i] = prefix + \" .\"\n",
    "    return \" \".join(tokens)  # Stitch the tokens back.\n",
    "\n",
    "text = \"This is a test sentence write on Sep. 6th. No. I am 123No. in all people. We are good at talk, swiming...\"\n",
    "text = replace_multidots(text)\n",
    "text = handles_nonbreaking_prefixes(text)\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理`'`号缩写\n",
    "在英文中，使用`'`号缩写非常常见，比如`I'm`,`You're`等等，他们其实是两个词，现在被缩写成了一个词，我们希望它被分割成`[\"I\", \"'m\"]`，`[\"You\", \"'re\"]`的形式。这个我们可以列几个简单的正则表达式进行处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'fine,', 'thank', 'you.', 'And', 'you?']\n"
     ]
    }
   ],
   "source": [
    "EN_SPECIFIC_1 = r\"([^{alpha}])[']([^{alpha}])\".format(alpha=IsAlpha), r\"\\1 ' \\2\"\n",
    "EN_SPECIFIC_2 = (\n",
    "    r\"([^{alpha}{isn}])[']([{alpha}])\".format(alpha=IsAlpha, isn=IsN),\n",
    "    r\"\\1 ' \\2\",\n",
    ")\n",
    "EN_SPECIFIC_3 = r\"([{alpha}])[']([^{alpha}])\".format(alpha=IsAlpha), r\"\\1 ' \\2\"\n",
    "EN_SPECIFIC_4 = r\"([{alpha}])[']([{alpha}])\".format(alpha=IsAlpha), r\"\\1 '\\2\"\n",
    "EN_SPECIFIC_5 = r\"([{isn}])[']([s])\".format(isn=IsN), r\"\\1 '\\2\"\n",
    "\n",
    "EN_SPECIFIC = [EN_SPECIFIC_1, EN_SPECIFIC_2, EN_SPECIFIC_3, EN_SPECIFIC_4, EN_SPECIFIC_5]\n",
    "\n",
    "text = \"I'm fine, thank you. And you?\"\n",
    "for regxp, substitution in EN_SPECIFIC:\n",
    "    text = re.sub(regxp, substitution, text)\n",
    "\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最后的收尾工作\n",
    "刚才在分词的过程中，还留下了两个遗留问题，一个是我们对连续多个`.`进行了Mask，现在要对其进行还原。二是在上述过程中可能会在词与词之间产生多个空格，我们要把它们合并成一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are apple , banana ...\n"
     ]
    }
   ],
   "source": [
    "def restore_multidots(text):\n",
    "    # 恢复对多个\".\"的mask\n",
    "    while re.search(r\"DOTDOTMULTI\", text):\n",
    "        text = re.sub(r\"DOTDOTMULTI\", r\"DOTMULTI.\", text)\n",
    "    return re.sub(r\"DOTMULTI\", r\".\", text)\n",
    "                  \n",
    "DEDUPLICATE_SPACE = r\"\\s+\", r\" \"\n",
    "regxp, substitution = DEDUPLICATE_SPACE\n",
    "\n",
    "text = \"There are apple  , banana  DOTDOTDOTMULTI\"\n",
    "text = restore_multidots(text)\n",
    "text = re.sub(regxp, substitution, text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可选处理项\n",
    "####  Mask 受保护的字符串\n",
    "在分词过程中，有一些固定的字符串格式（比如url，日期，时间等），我们不希望把他们拆分开，而是希望将他们标注为统一标识符，以便于在翻译过程中减少词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'webpage', 'THISISPROTECTED000', 'that', 'kicks', 'ass']\n"
     ]
    }
   ],
   "source": [
    "# 匹配<\\hello>标签\n",
    "BASIC_PROTECTED_PATTERN_1 = r\"<\\/?\\S+\\/?>\"\n",
    "\n",
    "# 匹配xml的标签  <hello=\"2\", hello2='3'>\n",
    "BASIC_PROTECTED_PATTERN_2 = r'<\\S+( [a-zA-Z0-9]+\\=\"?[^\"]\")+ ?\\/?>'\n",
    "BASIC_PROTECTED_PATTERN_3 = r\"<\\S+( [a-zA-Z0-9]+\\='?[^']')+ ?\\/?>\"\n",
    "# 匹配邮箱\n",
    "BASIC_PROTECTED_PATTERN_4 = r\"[\\w\\-\\_\\.]+\\@([\\w\\-\\_]+\\.)+[a-zA-Z]{2,}\"\n",
    "# 匹配url\n",
    "BASIC_PROTECTED_PATTERN_5 = r\"(http[s]?|ftp):\\/\\/[^:\\/\\s]+(\\/\\w+)*\\/[\\w\\-\\.]+\"\n",
    "\n",
    "BASIC_PROTECTED_PATTERNS = [\n",
    "        BASIC_PROTECTED_PATTERN_1,\n",
    "        BASIC_PROTECTED_PATTERN_2,\n",
    "        BASIC_PROTECTED_PATTERN_3,\n",
    "        BASIC_PROTECTED_PATTERN_4,\n",
    "        BASIC_PROTECTED_PATTERN_5,\n",
    "    ]\n",
    "\n",
    "text = \"this is a webpage https://stackoverflow.com/questions/6181381/how-to-print-variables-in-perl that kicks ass\"\n",
    "\n",
    "# Find the tokens that needs to be protected.\n",
    "protected_tokens = [\n",
    "    match.group()\n",
    "    for protected_pattern in BASIC_PROTECTED_PATTERNS\n",
    "    for match in re.finditer(protected_pattern, text, re.IGNORECASE)\n",
    "]\n",
    "# Apply the protected_patterns.\n",
    "for i, token in enumerate(protected_tokens):\n",
    "    substituition = \"THISISPROTECTED\" + str(i).zfill(3)\n",
    "    text = text.replace(token, substituition)\n",
    "    \n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分割破折号\n",
    "对于型如`word-word`的连字符号，我们可以选择将它们分成两个词，中间的破折号用特殊符号标记（方便Detokenize）。思路还是使用上面的IsAlnum字符表，如果存在某个破折号，两边都是IsAlnum中的字符，则将破折号与两边的字符用空格隔开。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence with hyphen. pre @-@ trained.\n"
     ]
    }
   ],
   "source": [
    "AGGRESSIVE_HYPHEN_SPLIT = (\n",
    "                  r\"([{alphanum}])\\-(?=[{alphanum}])\".format(alphanum=IsAlnum),\n",
    "                  r\"\\1 @-@ \",\n",
    "              )\n",
    "\n",
    "text = \"This is a sentence with hyphen. pre-trained.\"\n",
    "regxp, substitution = AGGRESSIVE_HYPHEN_SPLIT\n",
    "text = re.sub(regxp, substitution, text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def get_charset(charset_name):\n",
    "    f = open(os.path.join(\"assets\", charset_name + \".txt\"))\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_nobreaking_prefix(lang=\"en\"):\n",
    "    f = open(os.path.join(\"assets\", \"nonbreaking_prefix.\" + lang))\n",
    "    NONBREAKING_PREFIXES = []\n",
    "    NUMERIC_ONLY_PREFIXES = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            if line.endswith(\"#NUMERIC_ONLY#\"):\n",
    "                NUMERIC_ONLY_PREFIXES.append(line.split()[0])\n",
    "            if line not in NONBREAKING_PREFIXES:\n",
    "                NONBREAKING_PREFIXES.append(line)\n",
    "    f.close()\n",
    "    return NONBREAKING_PREFIXES, NUMERIC_ONLY_PREFIXES\n",
    "\n",
    "\n",
    "class MoseTokenizer(object):\n",
    "\n",
    "    # 字符集\n",
    "    IsAlnum = get_charset(\"IsAlnum\")\n",
    "    IsAlpha = get_charset(\"IsAlpha\")\n",
    "    IsLower = get_charset(\"IsLower\")\n",
    "    IsN = get_charset(\"IsN\")\n",
    "\n",
    "    # 步骤1 - 替换空白字符 相关正则表达式\n",
    "    DEDUPLICATE_SPACE = r\"\\s+\", r\" \"\n",
    "    ASCII_JUNK = r\"[\\000-\\037]\", r\"\"\n",
    "\n",
    "    # 步骤2 - 将常见标点、乱码等符号与词语分开 相关正则表达式\n",
    "    PAD_NOT_ISALNUM = r\"([^{}\\s\\.'\\`\\,\\-])\".format(IsAlnum), r\" \\1 \"\n",
    "\n",
    "    # 步骤4 - 分割逗号 相关正则表达式\n",
    "    # 若逗号前面不是数字，则分离逗号，如 hello,120 -> hello , 120\n",
    "    COMMA_SEPARATE_1 = r\"([^{}])[,]\".format(IsN), r\"\\1 , \"\n",
    "    # 若逗号后面不是数字，则分离逗号，如 120, hello -> 120 , hello\n",
    "    COMMA_SEPARATE_2 = r\"[,]([^{}])\".format(IsN), r\" , \\1\"\n",
    "    COMMA_SEPARATE_3 = r\"([{}])[,]$\".format(\n",
    "        IsN), r\"\\1 , \"  # 如果数字后匹配到结尾符，则分离逗号。 如120， -> 120 ,\n",
    "    COMMA_SEPARATE_4 = r\"^[,]([{}])\".format(\n",
    "        IsN), r\" \\1, \"  # 如果数字后匹配到结尾符，则分离逗号。 如120， -> 120 ,\n",
    "\n",
    "    COMMA_SEPARATE = [\n",
    "        COMMA_SEPARATE_1,\n",
    "        COMMA_SEPARATE_2,\n",
    "        COMMA_SEPARATE_3,\n",
    "        COMMA_SEPARATE_4\n",
    "    ]\n",
    "\n",
    "    # 步骤5 - 分割句号 受保护的前缀\n",
    "    NONBREAKING_PREFIXES, NUMERIC_ONLY_PREFIXES = get_nobreaking_prefix(\n",
    "        lang=\"en\")\n",
    "\n",
    "    # 步骤6 - 处理'号缩写 相关正则表达式\n",
    "    EN_SPECIFIC_1 = r\"([^{alpha}])[']([^{alpha}])\".format(\n",
    "        alpha=IsAlpha), r\"\\1 ' \\2\"\n",
    "    EN_SPECIFIC_2 = (\n",
    "        r\"([^{alpha}{isn}])[']([{alpha}])\".format(alpha=IsAlpha, isn=IsN),\n",
    "        r\"\\1 ' \\2\",\n",
    "    )\n",
    "    EN_SPECIFIC_3 = r\"([{alpha}])[']([^{alpha}])\".format(\n",
    "        alpha=IsAlpha), r\"\\1 ' \\2\"\n",
    "    EN_SPECIFIC_4 = r\"([{alpha}])[']([{alpha}])\".format(\n",
    "        alpha=IsAlpha), r\"\\1 '\\2\"\n",
    "    EN_SPECIFIC_5 = r\"([{isn}])[']([s])\".format(isn=IsN), r\"\\1 '\\2\"\n",
    "\n",
    "    EN_SPECIFIC = [\n",
    "        EN_SPECIFIC_1,\n",
    "        EN_SPECIFIC_1,\n",
    "        EN_SPECIFIC_1,\n",
    "        EN_SPECIFIC_1,\n",
    "        EN_SPECIFIC_1\n",
    "    ]\n",
    "\n",
    "    # 可选步骤 Mask 受保护的字符串 相关的正则表达式\n",
    "    BASIC_PROTECTED_PATTERN_1 = r\"<\\/?\\S+\\/?>\"\n",
    "    BASIC_PROTECTED_PATTERN_2 = r'<\\S+( [a-zA-Z0-9]+\\=\"?[^\"]\")+ ?\\/?>'\n",
    "    BASIC_PROTECTED_PATTERN_3 = r\"<\\S+( [a-zA-Z0-9]+\\='?[^']')+ ?\\/?>\"\n",
    "    BASIC_PROTECTED_PATTERN_4 = r\"[\\w\\-\\_\\.]+\\@([\\w\\-\\_]+\\.)+[a-zA-Z]{2,}\"\n",
    "    BASIC_PROTECTED_PATTERN_5 = r\"(http[s]?|ftp):\\/\\/[^:\\/\\s]+(\\/\\w+)*\\/[\\w\\-\\.]+\"\n",
    "\n",
    "    BASIC_PROTECTED_PATTERNS = [\n",
    "        BASIC_PROTECTED_PATTERN_1,\n",
    "        BASIC_PROTECTED_PATTERN_2,\n",
    "        BASIC_PROTECTED_PATTERN_3,\n",
    "        BASIC_PROTECTED_PATTERN_4,\n",
    "        BASIC_PROTECTED_PATTERN_5\n",
    "    ]\n",
    "\n",
    "    # 可选步骤 分割破折号 相关正则表达式\n",
    "    AGGRESSIVE_HYPHEN_SPLIT = (\n",
    "        r\"([{alphanum}])\\-(?=[{alphanum}])\".format(alphanum=IsAlnum),\n",
    "        r\"\\1 @-@ \",\n",
    "    )\n",
    "\n",
    "    def isanyalpha(self, text):\n",
    "        # 判断给定字符串中是否全是字母（非数字、符号）\n",
    "        return any(set(text).intersection(set(self.IsAlpha)))\n",
    "\n",
    "    def islower(self, text):\n",
    "        # 判断给定字符串中是否全部都是小写字母\n",
    "        return not set(text).difference(set(self.IsLower))\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_multidots(text):\n",
    "        # 处理情况1，对多个\".\"的情况作mask处理\n",
    "        text = re.sub(r\"\\.([\\.]+)\", r\" DOTMULTI\\1\", text)\n",
    "        while re.search(r\"DOTMULTI\\.\", text):\n",
    "            text = re.sub(r\"DOTMULTI\\.([^\\.])\", r\"DOTDOTMULTI \\1\", text)\n",
    "            text = re.sub(r\"DOTMULTI\\.\", \"DOTDOTMULTI\", text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def restore_multidots(text):\n",
    "        # 恢复对多个\".\"的mask\n",
    "        while re.search(r\"DOTDOTMULTI\", text):\n",
    "            text = re.sub(r\"DOTDOTMULTI\", r\"DOTMULTI.\", text)\n",
    "        return re.sub(r\"DOTMULTI\", r\".\", text)\n",
    "\n",
    "    def handles_nonbreaking_prefixes(self, text):\n",
    "        # 将文本拆分为标记以检查 \".\" 为结尾的部分是否符合拆分条件\n",
    "        tokens = text.split()\n",
    "        num_tokens = len(tokens)\n",
    "        for i, token in enumerate(tokens):\n",
    "            # 判断是否以\".\"结尾\n",
    "            token_ends_with_period = re.search(r\"^(\\S+)\\.$\", token)\n",
    "            if token_ends_with_period:\n",
    "                prefix = token_ends_with_period.group(1)\n",
    "\n",
    "                # 处理情况2,3,4,6\n",
    "                if (\n",
    "                    (\".\" in prefix and self.isanyalpha(prefix))\n",
    "                    or (\n",
    "                        prefix in self.NONBREAKING_PREFIXES\n",
    "                        and prefix not in self.NUMERIC_ONLY_PREFIXES\n",
    "                    )\n",
    "                    or (\n",
    "                        i != num_tokens - 1\n",
    "                        and tokens[i + 1]\n",
    "                        and self.islower(tokens[i + 1][0])\n",
    "                    )\n",
    "                ):\n",
    "                    pass  # 不做拆分处理\n",
    "\n",
    "                # 处理情况 5\n",
    "                elif (\n",
    "                    prefix in self.NUMERIC_ONLY_PREFIXES\n",
    "                    and (i + 1) < num_tokens\n",
    "                    and re.search(r\"^[0-9]+\", tokens[i + 1])\n",
    "                ):\n",
    "                    pass  # 不做拆分处理\n",
    "                else:  # 不在1-6中，做拆分处理\n",
    "                    tokens[i] = prefix + \" .\"\n",
    "        return \" \".join(tokens)  # Stitch the tokens back.\n",
    "\n",
    "    def tokenize(self,\n",
    "                 text,\n",
    "                 aggressive_dash_splits=False,  # 是否分割破折号 \"-\"\n",
    "                 return_str=False,  # 返回字符串还是以list的形式返回\n",
    "                 protected_patterns=None  # Mask 受保护的字符串 （以正则表达式list的形式传入）\n",
    "                 ):\n",
    "\n",
    "        # 步骤1 - 替换空白字符\n",
    "        for regexp, substitution in [self.DEDUPLICATE_SPACE, self.ASCII_JUNK]:\n",
    "            text = re.sub(regexp, substitution, text)\n",
    "\n",
    "        # 可选步骤 Mask 受保护的字符串\n",
    "        if protected_patterns:\n",
    "            protecte_partterns.extend(self.BASIC_PROTECTED_PATTERNS)\n",
    "        else:\n",
    "            protecte_partterns = self.BASIC_PROTECTED_PATTERNS\n",
    "\n",
    "        # Find the tokens that needs to be protected.\n",
    "        protected_tokens = [\n",
    "            match.group()\n",
    "            for protected_pattern in self.BASIC_PROTECTED_PATTERNS\n",
    "            for match in re.finditer(protected_pattern, text, re.IGNORECASE)\n",
    "        ]\n",
    "        # Apply the protected_patterns.\n",
    "        for i, token in enumerate(protected_tokens):\n",
    "            substituition = \"THISISPROTECTED\" + str(i).zfill(3)\n",
    "            text = text.replace(token, substituition)\n",
    "\n",
    "        # 步骤2 - 将常见标点、乱码等符号与词语分开 相关正则表达式\n",
    "        regxp, substitution = self.PAD_NOT_ISALNUM\n",
    "        text = re.sub(regxp, substitution, text)\n",
    "\n",
    "        # 步骤3 - 去掉句子开头和结尾的空白字符\n",
    "        text = text.strip()\n",
    "\n",
    "        # 步骤4 - 分割逗号\n",
    "        for regxp, substitution in self.COMMA_SEPARATE:\n",
    "            text = re.sub(regxp, substitution, text)\n",
    "\n",
    "        # 步骤5 - 分割句号\n",
    "        text = self.replace_multidots(text)\n",
    "        text = self.handles_nonbreaking_prefixes(text)\n",
    "\n",
    "        # 步骤6 - 处理'号缩写\n",
    "        for regxp, substitution in self.EN_SPECIFIC:\n",
    "            text = re.sub(regxp, substitution, text)\n",
    "\n",
    "        if aggressive_dash_splits:\n",
    "            regxp, substitution = self.AGGRESSIVE_HYPHEN_SPLIT\n",
    "            text = re.sub(regxp, substitution, text)\n",
    "\n",
    "        # 收尾工作\n",
    "        regxp, substitution = self.DEDUPLICATE_SPACE\n",
    "        text = self.restore_multidots(text)\n",
    "        text = re.sub(regxp, substitution, text)\n",
    "\n",
    "        # 恢复受保护的字符串 Mask->原字符串.\n",
    "        for i, token in enumerate(protected_tokens):\n",
    "            substituition = \"THISISPROTECTED\" + str(i).zfill(3)\n",
    "            text = text.replace(substituition, token)\n",
    "\n",
    "        return text if return_str else text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'webpage', 'https://stackoverflow.com/questions/6181381/how-to-print-variables-in-perl', 'that', 'kicks', 'ass']\n",
      "['Sie', 'sollten', 'vor', 'dem', 'Upgrade', 'eine', 'Sicherung', 'dieser', 'Daten', 'erstellen', '(', 'wie', 'unter', 'Abschnitt', '4.1.1', ',', '„', 'Sichern', 'aller', 'Daten', 'und', 'Konfigurationsinformationen', '“', 'beschrieben', ')', '.']\n",
      "['This', \"ain't\", 'funny', '.', \"It's\", 'actually', 'hillarious', ',', 'yet', 'double', 'Ls', '.', '|', '[', ']', '<', '>', '[', ']', '&', \"You're\", 'gonna', 'shake', 'it', 'off', '?', \"Don't\", '?']\n",
      "['This', ',', 'is', 'a', 'sentence', 'with', 'weird', '»', 'symbols', '…', 'appearing', 'everywhere', '¿']\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"this is a webpage https://stackoverflow.com/questions/6181381/how-to-print-variables-in-perl that kicks ass\",\n",
    "    \"Sie sollten vor dem Upgrade eine Sicherung dieser Daten erstellen (wie unter Abschnitt 4.1.1, „Sichern aller Daten und Konfigurationsinformationen“ beschrieben).\",\n",
    "    \"This ain't funny. It's actually hillarious, yet double Ls. | [] < > [ ] & You're gonna shake it off? Don't?\",\n",
    "    \"This, is a sentence with weird\\xbb symbols\\u2026 appearing everywhere\\xbf\"\n",
    "]\n",
    "\n",
    "mt = MoseTokenizer()\n",
    "\n",
    "for text in test_sentences:\n",
    "    text = mt.tokenize(text)\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('pytorch_latest': conda)",
   "language": "python",
   "name": "python37364bitpytorchlatestconda37dda3a0837247e597f023e05705e960"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
